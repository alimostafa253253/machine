{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:53:58.064916Z",
     "iopub.status.busy": "2025-03-27T05:53:58.064523Z",
     "iopub.status.idle": "2025-03-27T05:54:04.493912Z",
     "shell.execute_reply": "2025-03-27T05:54:04.492415Z",
     "shell.execute_reply.started": "2025-03-27T05:53:58.064890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in d:\\venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\venv\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in d:\\venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\venv\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\venv\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\venv\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\venv\\lib\\site-packages (from evaluate) (0.29.1)\n",
      "Requirement already satisfied: packaging in d:\\venv\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\venv\\lib\\site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\venv\\lib\\site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:54:04.495885Z",
     "iopub.status.busy": "2025-03-27T05:54:04.495549Z",
     "iopub.status.idle": "2025-03-27T05:54:31.601760Z",
     "shell.execute_reply": "2025-03-27T05:54:31.600484Z",
     "shell.execute_reply.started": "2025-03-27T05:54:04.495854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:54:31.604450Z",
     "iopub.status.busy": "2025-03-27T05:54:31.604110Z",
     "iopub.status.idle": "2025-03-27T05:55:03.444729Z",
     "shell.execute_reply": "2025-03-27T05:55:03.443584Z",
     "shell.execute_reply.started": "2025-03-27T05:54:31.604381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import re\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:03.447164Z",
     "iopub.status.busy": "2025-03-27T05:55:03.446291Z",
     "iopub.status.idle": "2025-03-27T05:55:09.351577Z",
     "shell.execute_reply": "2025-03-27T05:55:09.350461Z",
     "shell.execute_reply.started": "2025-03-27T05:55:03.447132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdelaziz67\u001b[0m (\u001b[33mabdelaziz67-ain-shams-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your WandB API key directly\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4cf3591f262cd568777e73fcda947286ee03b410\"\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# Initialize WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:09.353748Z",
     "iopub.status.busy": "2025-03-27T05:55:09.352719Z",
     "iopub.status.idle": "2025-03-27T05:55:15.457269Z",
     "shell.execute_reply": "2025-03-27T05:55:15.456257Z",
     "shell.execute_reply.started": "2025-03-27T05:55:09.353703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\projects\\shabab_mobtakren\\abdelaziz\\wandb\\run-20250329_170152-ml3b000a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a' target=\"_blank\">peach-forest-6</a></strong> to <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x194d6f512a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🎯 Start a new WandB run\n",
    "wandb.init(\n",
    "    project=\"egyptian-english-translation-finetuning\",\n",
    "    entity=\"abdelaziz67-ain-shams-university\",  # Your WandB username or organization name\n",
    "    config={\n",
    "        \"model_checkpoint\": \"Helsinki-NLP/opus-mt-en-ar\",\n",
    "        # \"model_checkpoint\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "        \"learning_rate\": 4e-5,  # Lower LR for stable fine-tuning\n",
    "        \"batch_size\": 12,  # Increased batch size\n",
    "        \"num_train_epochs\": 35,  # Extended training duration\n",
    "        \"seed\": 42,\n",
    "        \"label_smoothing\": 0.1  # Prevent overconfidence\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.458569Z",
     "iopub.status.busy": "2025-03-27T05:55:15.458264Z",
     "iopub.status.idle": "2025-03-27T05:55:15.463816Z",
     "shell.execute_reply": "2025-03-27T05:55:15.462526Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.458543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.465443Z",
     "iopub.status.busy": "2025-03-27T05:55:15.465049Z",
     "iopub.status.idle": "2025-03-27T05:55:15.485820Z",
     "shell.execute_reply": "2025-03-27T05:55:15.483992Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.465382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 🪵 Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.489221Z",
     "iopub.status.busy": "2025-03-27T05:55:15.488892Z",
     "iopub.status.idle": "2025-03-27T05:55:15.516029Z",
     "shell.execute_reply": "2025-03-27T05:55:15.514911Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.489197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "set_seed(wandb.config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.517666Z",
     "iopub.status.busy": "2025-03-27T05:55:15.517281Z",
     "iopub.status.idle": "2025-03-27T05:55:17.121322Z",
     "shell.execute_reply": "2025-03-27T05:55:17.120359Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.517639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading dataset from Hugging Face Hub...\n",
      "INFO:__main__:Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['EGY', 'ENG'],\n",
      "        num_rows: 26047\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading dataset from Hugging Face Hub...\")\n",
    "dataset = load_dataset(\"HeshamHaroon/ArzEn-MultiGenre\")\n",
    "logger.info(\"Dataset loaded: %s\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:56:08.401916Z",
     "iopub.status.busy": "2025-03-27T05:56:08.401381Z",
     "iopub.status.idle": "2025-03-27T05:56:08.409259Z",
     "shell.execute_reply": "2025-03-27T05:56:08.407989Z",
     "shell.execute_reply.started": "2025-03-27T05:56:08.401882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['EGY', 'ENG'],\n",
      "        num_rows: 26047\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'EGY': '‫لحق؟‬', 'ENG': 'Already?'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'EGY': '‫معلش يا جماعة أخرتكم.‬', 'ENG': 'Sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'EGY': '‫لا، ولا يهمك.‬', 'ENG': 'No problem.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'EGY': '‫بس الsystem down.‬', 'ENG': 'The sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'EGY': '‫طيب. خلاص إحنا كدا قفلنا الjoint acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'EGY': '‫كل ده والsystem down؟‬', 'ENG': 'All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'EGY': '‫لا إنتي بتصنعي المعجزات يا سها طول ع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'EGY': '‫لا خالص. إحنا بس اتعودنا.‬', 'ENG': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'EGY': '‫من كتر حالات الطلاق ومشاكل النفقة.‬ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'EGY': '‫أنا بس عشان عارف إن علا مش بتشتغل‬ ‫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'EGY': '‫أنا كمان أنا عرضت عليها مصروف شخصي ش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'EGY': '‫لو هي تحب يعني أنا ممكن أعمل كدا‬، م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'EGY': '‫وأنا قولتلك إن أنا مش هاخد مصروف‬ ‫ع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'EGY': '‫أنا هاخد مصروف الولاد وبس.‬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'EGY': '‫يعني هتصرفي على نفسك منين؟‬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'EGY': '‫هشتغل.‬', 'ENG': 'I'll work.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'EGY': 'هتشتغلي إيه يا علا؟‬', 'ENG': 'Doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'EGY': '‫ده fresh graduates مش لاقيين.‬', 'EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'EGY': '‫مش أنا قولتلك؟‬ ‫أنا قولتلك من الأول...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'EGY': '‫هو مش ابن خالتي؟ بس مفيش راجل يستاهل...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                train\n",
       "0                {'EGY': '‫لحق؟‬', 'ENG': 'Already?'}\n",
       "1   {'EGY': '‫معلش يا جماعة أخرتكم.‬', 'ENG': 'Sor...\n",
       "2    {'EGY': '‫لا، ولا يهمك.‬', 'ENG': 'No problem.'}\n",
       "3   {'EGY': '‫بس الsystem down.‬', 'ENG': 'The sys...\n",
       "4   {'EGY': '‫طيب. خلاص إحنا كدا قفلنا الjoint acc...\n",
       "5   {'EGY': '‫كل ده والsystem down؟‬', 'ENG': 'All...\n",
       "6   {'EGY': '‫لا إنتي بتصنعي المعجزات يا سها طول ع...\n",
       "7   {'EGY': '‫لا خالص. إحنا بس اتعودنا.‬', 'ENG': ...\n",
       "8   {'EGY': '‫من كتر حالات الطلاق ومشاكل النفقة.‬ ...\n",
       "9   {'EGY': '‫أنا بس عشان عارف إن علا مش بتشتغل‬ ‫...\n",
       "10  {'EGY': '‫أنا كمان أنا عرضت عليها مصروف شخصي ش...\n",
       "11  {'EGY': '‫لو هي تحب يعني أنا ممكن أعمل كدا‬، م...\n",
       "12  {'EGY': '‫وأنا قولتلك إن أنا مش هاخد مصروف‬ ‫ع...\n",
       "13  {'EGY': '‫أنا هاخد مصروف الولاد وبس.‬', 'ENG':...\n",
       "14  {'EGY': '‫يعني هتصرفي على نفسك منين؟‬', 'ENG':...\n",
       "15           {'EGY': '‫هشتغل.‬', 'ENG': 'I'll work.'}\n",
       "16  {'EGY': 'هتشتغلي إيه يا علا؟‬', 'ENG': 'Doing ...\n",
       "17  {'EGY': '‫ده fresh graduates مش لاقيين.‬', 'EN...\n",
       "18  {'EGY': '‫مش أنا قولتلك؟‬ ‫أنا قولتلك من الأول...\n",
       "19  {'EGY': '‫هو مش ابن خالتي؟ بس مفيش راجل يستاهل..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the cleaned dataset\n",
    "print(dataset)\n",
    "import pandas as pd\n",
    "dt=pd.DataFrame(dataset)\n",
    "dt.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:56:40.017931Z",
     "iopub.status.busy": "2025-03-27T05:56:40.017480Z",
     "iopub.status.idle": "2025-03-27T05:56:40.026027Z",
     "shell.execute_reply": "2025-03-27T05:56:40.024055Z",
     "shell.execute_reply.started": "2025-03-27T05:56:40.017903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,!?؛،]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:56:46.404637Z",
     "iopub.status.busy": "2025-03-27T05:56:46.404140Z",
     "iopub.status.idle": "2025-03-27T05:56:46.419431Z",
     "shell.execute_reply": "2025-03-27T05:56:46.418098Z",
     "shell.execute_reply.started": "2025-03-27T05:56:46.404603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['EGY', 'ENG'],\n",
      "        num_rows: 22466\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a value is None or only numbers\n",
    "def is_invalid(value):\n",
    "    return value is None or (isinstance(value, str) and value.strip().isdigit())\n",
    "\n",
    "# Function to filter dataset\n",
    "def filter_rows(example):\n",
    "    return not any(is_invalid(v) for v in example.values())\n",
    "\n",
    "# Apply filter to all splits\n",
    "dataset_cleaned = dataset.filter(filter_rows)\n",
    "\n",
    "# Check the cleaned dataset\n",
    "print(dataset_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'EGY': '‫لحق؟‬', 'ENG': 'Already?'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'EGY': '‫معلش يا جماعة أخرتكم.‬', 'ENG': 'Sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'EGY': '‫لا، ولا يهمك.‬', 'ENG': 'No problem.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'EGY': '‫بس الsystem down.‬', 'ENG': 'The sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'EGY': '‫طيب. خلاص إحنا كدا قفلنا الjoint acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'EGY': '‫كل ده والsystem down؟‬', 'ENG': 'All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'EGY': '‫لا إنتي بتصنعي المعجزات يا سها طول ع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'EGY': '‫لا خالص. إحنا بس اتعودنا.‬', 'ENG': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'EGY': '‫من كتر حالات الطلاق ومشاكل النفقة.‬ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'EGY': '‫أنا بس عشان عارف إن علا مش بتشتغل‬ ‫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'EGY': '‫أنا كمان أنا عرضت عليها مصروف شخصي ش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'EGY': '‫لو هي تحب يعني أنا ممكن أعمل كدا‬، م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'EGY': '‫وأنا قولتلك إن أنا مش هاخد مصروف‬ ‫ع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'EGY': '‫أنا هاخد مصروف الولاد وبس.‬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'EGY': '‫يعني هتصرفي على نفسك منين؟‬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'EGY': '‫هشتغل.‬', 'ENG': 'I'll work.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'EGY': 'هتشتغلي إيه يا علا؟‬', 'ENG': 'Doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'EGY': '‫ده fresh graduates مش لاقيين.‬', 'EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'EGY': '‫مش أنا قولتلك؟‬ ‫أنا قولتلك من الأول...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'EGY': '‫هو مش ابن خالتي؟ بس مفيش راجل يستاهل...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                train\n",
       "0                {'EGY': '‫لحق؟‬', 'ENG': 'Already?'}\n",
       "1   {'EGY': '‫معلش يا جماعة أخرتكم.‬', 'ENG': 'Sor...\n",
       "2    {'EGY': '‫لا، ولا يهمك.‬', 'ENG': 'No problem.'}\n",
       "3   {'EGY': '‫بس الsystem down.‬', 'ENG': 'The sys...\n",
       "4   {'EGY': '‫طيب. خلاص إحنا كدا قفلنا الjoint acc...\n",
       "5   {'EGY': '‫كل ده والsystem down؟‬', 'ENG': 'All...\n",
       "6   {'EGY': '‫لا إنتي بتصنعي المعجزات يا سها طول ع...\n",
       "7   {'EGY': '‫لا خالص. إحنا بس اتعودنا.‬', 'ENG': ...\n",
       "8   {'EGY': '‫من كتر حالات الطلاق ومشاكل النفقة.‬ ...\n",
       "9   {'EGY': '‫أنا بس عشان عارف إن علا مش بتشتغل‬ ‫...\n",
       "10  {'EGY': '‫أنا كمان أنا عرضت عليها مصروف شخصي ش...\n",
       "11  {'EGY': '‫لو هي تحب يعني أنا ممكن أعمل كدا‬، م...\n",
       "12  {'EGY': '‫وأنا قولتلك إن أنا مش هاخد مصروف‬ ‫ع...\n",
       "13  {'EGY': '‫أنا هاخد مصروف الولاد وبس.‬', 'ENG':...\n",
       "14  {'EGY': '‫يعني هتصرفي على نفسك منين؟‬', 'ENG':...\n",
       "15           {'EGY': '‫هشتغل.‬', 'ENG': 'I'll work.'}\n",
       "16  {'EGY': 'هتشتغلي إيه يا علا؟‬', 'ENG': 'Doing ...\n",
       "17  {'EGY': '‫ده fresh graduates مش لاقيين.‬', 'EN...\n",
       "18  {'EGY': '‫مش أنا قولتلك؟‬ ‫أنا قولتلك من الأول...\n",
       "19  {'EGY': '‫هو مش ابن خالتي؟ بس مفيش راجل يستاهل..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(dataset_cleaned)\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T06:00:41.492441Z",
     "iopub.status.busy": "2025-03-27T06:00:41.491940Z",
     "iopub.status.idle": "2025-03-27T06:00:41.521724Z",
     "shell.execute_reply": "2025-03-27T06:00:41.520072Z",
     "shell.execute_reply.started": "2025-03-27T06:00:41.492385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets if not already split.\n",
    "if \"train\" not in dataset_cleaned.keys() or \"validation\" not in dataset_cleaned.keys():\n",
    "    dataset_cleaned = dataset_cleaned[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = dataset_cleaned[\"train\"]\n",
    "    val_dataset = dataset_cleaned[\"test\"]\n",
    "else:\n",
    "    train_dataset = dataset_cleaned[\"train\"]\n",
    "    val_dataset = dataset_cleaned[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T06:00:44.968039Z",
     "iopub.status.busy": "2025-03-27T06:00:44.967599Z",
     "iopub.status.idle": "2025-03-27T06:00:45.026600Z",
     "shell.execute_reply": "2025-03-27T06:00:45.025355Z",
     "shell.execute_reply.started": "2025-03-27T06:00:44.968008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Source: بصلي بطريقة غريبة، زي ما أكون خليته يحس بشوية إشمئزاز. وقالي بطريقة أقرب للعنف إن في كل الأحوال شهادة مدير وموظفين الدار هتتسمع وإن \"ده ممكن يقلب عليا التربيزة بطريقة وحشة\". \n",
      "Target: He gave me a queer look, as if I slightly revolted him; then informed me, in an almost hostile tone, that in any case the head of the Home and some of the staff would be cited as witnesses. And that might do you a very nasty turn, he concluded. \n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Source: مكنش مفروض أقول اللي قولته ده خالص.\n",
      "Target: I shouldn't have said those things.\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Source: اطلعيلي فوق عشان أرشق الشوكه فيكي. \n",
      "Target: Come up easy and let me put the harpoon into you.\n",
      "--------------------------------------------------\n",
      "Example 4:\n",
      "Source: رفعت\n",
      "Target: Refaat!\n",
      "--------------------------------------------------\n",
      "Example 5:\n",
      "Source: المركب كانت خفيفه دلوقتي وماكانش عنده أي أفكار ولا مشاعر من أي نوع. \n",
      "Target: He sailed lightly now and he had no thoughts nor any feelings of any kind.\n",
      "--------------------------------------------------\n",
      "Example 6:\n",
      "Source: بقيت شكري سرحان\n",
      "Target: I've become as famous as Shoukry Sarhan.\n",
      "--------------------------------------------------\n",
      "Example 7:\n",
      "Source: هشيل حاجاتي بس في ثانية.\n",
      "Target: I'll just move my stuff. Hold on.\n",
      "--------------------------------------------------\n",
      "Example 8:\n",
      "Source: ودي مش بإيدي\n",
      "Target: It is out of my hands\n",
      "--------------------------------------------------\n",
      "Example 9:\n",
      "Source: نجرب إيه؟ \n",
      "Target: Hell, no!\n",
      "--------------------------------------------------\n",
      "Example 10:\n",
      "Source: كان شايل قايمه بخيل السبق في جيبه طول الوقت وكان بيقول أسماء الحصنه على التليفون كتير.\n",
      "Target: At least he carried lists of horses at all times in his pocket and frequently spoke the names of horses on the telephone. \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show a few examples (e.g., first 5)\n",
    "for idx, example in enumerate(train_dataset.select(range(100,110))):\n",
    "    print(f\"Example {idx+1}:\")\n",
    "    print(\"Source:\", example.get(\"EGY\", \"Field 'tgt' not found\"))\n",
    "    print(\"Target:\", example.get(\"ENG\", \"Field 'src' not found\"))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:52.732652Z",
     "iopub.status.busy": "2025-03-26T13:16:52.732340Z",
     "iopub.status.idle": "2025-03-26T13:16:52.736968Z",
     "shell.execute_reply": "2025-03-26T13:16:52.736107Z",
     "shell.execute_reply.started": "2025-03-26T13:16:52.732621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset loaded successfully!\n",
      "INFO:__main__:Training samples: 20219\n",
      "INFO:__main__:Validation samples: 2247\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Dataset loaded successfully!\")\n",
    "logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:52.738320Z",
     "iopub.status.busy": "2025-03-26T13:16:52.737817Z",
     "iopub.status.idle": "2025-03-26T13:16:57.368173Z",
     "shell.execute_reply": "2025-03-26T13:16:57.366217Z",
     "shell.execute_reply.started": "2025-03-26T13:16:52.738285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Choose a pre-trained translation model\n",
    "model_checkpoint = wandb.config.model_checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.369957Z",
     "iopub.status.busy": "2025-03-26T13:16:57.369610Z",
     "iopub.status.idle": "2025-03-26T13:16:57.375094Z",
     "shell.execute_reply": "2025-03-26T13:16:57.374297Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.369928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.config.dropout = 0.2\n",
    "model.config.attention_dropout = 0.2\n",
    "model.config.activation_dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.376279Z",
     "iopub.status.busy": "2025-03-26T13:16:57.375978Z",
     "iopub.status.idle": "2025-03-26T13:16:57.396590Z",
     "shell.execute_reply": "2025-03-26T13:16:57.396013Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.376249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define source and target languages (adjust codes as needed)\n",
    "source_lang = \"ar_EG\" # for English (if translating from English to Egyptian Arabic)\n",
    "target_lang = \"en_XX\"# Egyptian Arabic variant (if supported)\n",
    "# Note: If your target dialect is not explicitly supported, you can still fine-tune the model\n",
    "# with your dialect-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.397827Z",
     "iopub.status.busy": "2025-03-26T13:16:57.397561Z",
     "iopub.status.idle": "2025-03-26T13:16:57.417030Z",
     "shell.execute_reply": "2025-03-26T13:16:57.416131Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.397806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Ensure inputs are strings\n",
    "    inputs = [clean_text(str(text)) for text in examples[\"EGY\"]]\n",
    "    targets = [clean_text(str(text)) for text in examples[\"ENG\"]]\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # Tokenize the target texts using the new text_target argument\n",
    "    labels = tokenizer(text_target=targets, max_length=max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Before tokenizing, set the source and target language codes on the tokenizer.\n",
    "# For mBART50, for example:\n",
    "tokenizer.src_lang = \"ar_EG\"  # assuming source is English\n",
    "tokenizer.tgt_lang = \"en_XX\"  # assuming target is Arabic; change if you have a dialect-specific code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.418228Z",
     "iopub.status.busy": "2025-03-26T13:16:57.417880Z",
     "iopub.status.idle": "2025-03-26T13:17:03.532391Z",
     "shell.execute_reply": "2025-03-26T13:17:03.531399Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.418192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:03.533423Z",
     "iopub.status.busy": "2025-03-26T13:17:03.533206Z",
     "iopub.status.idle": "2025-03-26T13:17:03.537336Z",
     "shell.execute_reply": "2025-03-26T13:17:03.536578Z",
     "shell.execute_reply.started": "2025-03-26T13:17:03.533404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:03.538411Z",
     "iopub.status.busy": "2025-03-26T13:17:03.538182Z",
     "iopub.status.idle": "2025-03-26T13:17:03.713884Z",
     "shell.execute_reply": "2025-03-26T13:17:03.713193Z",
     "shell.execute_reply.started": "2025-03-26T13:17:03.538392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,  # Keep only the last 2 checkpoints,\n",
    "    save_only_model=True,   # Save only the model\n",
    "    per_device_train_batch_size=wandb.config.batch_size,\n",
    "    per_device_eval_batch_size=wandb.config.batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=wandb.config.num_train_epochs,\n",
    "    learning_rate=wandb.config.learning_rate,\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate decay,\n",
    "    weight_decay=0.03,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,  # Limit generation length\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    label_smoothing_factor=wandb.config.label_smoothing,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"optimized-egyptian-arabic-translation\",\n",
    "    fp16=True  # Enable mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:03.714973Z",
     "iopub.status.busy": "2025-03-26T13:17:03.714774Z",
     "iopub.status.idle": "2025-03-26T13:17:07.679726Z",
     "shell.execute_reply": "2025-03-26T13:17:07.678814Z",
     "shell.execute_reply.started": "2025-03-26T13:17:03.714955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in d:\\venv\\lib\\site-packages (from sacrebleu) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\venv\\lib\\site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in d:\\venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in d:\\venv\\lib\\site-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\venv\\lib\\site-packages (from portalocker->sacrebleu) (308)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:07.681332Z",
     "iopub.status.busy": "2025-03-26T13:17:07.680928Z",
     "iopub.status.idle": "2025-03-26T13:17:08.175903Z",
     "shell.execute_reply": "2025-03-26T13:17:08.175139Z",
     "shell.execute_reply.started": "2025-03-26T13:17:07.681280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the sacreBLEU metric using the evaluate library.\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.177763Z",
     "iopub.status.busy": "2025-03-26T13:17:08.176677Z",
     "iopub.status.idle": "2025-03-26T13:17:08.183753Z",
     "shell.execute_reply": "2025-03-26T13:17:08.182768Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.177736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Safely convert predictions to integers\n",
    "    preds = preds.astype(object)\n",
    "    labels = labels.astype(object)\n",
    "\n",
    "    # Decode the generated texts and labels directly using the tokenizer\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace all -100 in labels with the padding token ID\n",
    "    labels = [[(int(token) if token >= 0 else tokenizer.pad_token_id) for token in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Prepare references in the required format\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    # Compute BLEU score using sacrebleu\n",
    "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Log BLEU score to wandb\n",
    "    wandb.log({\"BLEU score\": result[\"score\"]})\n",
    "\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.185022Z",
     "iopub.status.busy": "2025-03-26T13:17:08.184705Z",
     "iopub.status.idle": "2025-03-26T13:17:08.207754Z",
     "shell.execute_reply": "2025-03-26T13:17:08.206971Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.184991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.208630Z",
     "iopub.status.busy": "2025-03-26T13:17:08.208414Z",
     "iopub.status.idle": "2025-03-26T13:17:08.226887Z",
     "shell.execute_reply": "2025-03-26T13:17:08.226063Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.208611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.227803Z",
     "iopub.status.busy": "2025-03-26T13:17:08.227618Z",
     "iopub.status.idle": "2025-03-26T13:17:08.245039Z",
     "shell.execute_reply": "2025-03-26T13:17:08.244270Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.227786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def clear_memory():\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# # Call this after each evaluation step or at the end of training\n",
    "# clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.248866Z",
     "iopub.status.busy": "2025-03-26T13:17:08.248670Z",
     "iopub.status.idle": "2025-03-26T13:17:08.612267Z",
     "shell.execute_reply": "2025-03-26T13:17:08.611547Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.248849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,  # Already handles tokenization and padding\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['EGY', 'ENG', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2247\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.613636Z",
     "iopub.status.busy": "2025-03-26T13:17:08.613379Z",
     "iopub.status.idle": "2025-03-26T18:40:40.145026Z",
     "shell.execute_reply": "2025-03-26T18:40:40.144122Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.613614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='14735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  237/14735 32:55 < 33:50:53, 0.12 it/s, Epoch 0.56/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start fine-tuning\n",
    "logger.info(\"Starting fine-tuning...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:40:40.146308Z",
     "iopub.status.busy": "2025-03-26T18:40:40.145973Z",
     "iopub.status.idle": "2025-03-26T18:42:26.943310Z",
     "shell.execute_reply": "2025-03-26T18:42:26.942663Z",
     "shell.execute_reply.started": "2025-03-26T18:40:40.146256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 01:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "logger.info(\"Evaluating the model on the validation set...\")\n",
    "evaluation_results = trainer.evaluate()\n",
    "logger.info(\"Evaluation results: %s\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:26.944267Z",
     "iopub.status.busy": "2025-03-26T18:42:26.944060Z",
     "iopub.status.idle": "2025-03-26T18:42:27.716695Z",
     "shell.execute_reply": "2025-03-26T18:42:27.716041Z",
     "shell.execute_reply.started": "2025-03-26T18:42:26.944248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "logger.info(\"Model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:27.717640Z",
     "iopub.status.busy": "2025-03-26T18:42:27.717410Z",
     "iopub.status.idle": "2025-03-26T18:42:28.924686Z",
     "shell.execute_reply": "2025-03-26T18:42:28.923927Z",
     "shell.execute_reply.started": "2025-03-26T18:42:27.717619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./final_model)... Done. 1.1s\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Log the model as a WandB artifact\n",
    "artifact = wandb.Artifact(\"translation_model\", type=\"model\")\n",
    "artifact.add_dir(\"./final_model\")\n",
    "wandb.log_artifact(artifact)\n",
    "logger.info(\"Model saved and logged to WandB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:28.925715Z",
     "iopub.status.busy": "2025-03-26T18:42:28.925441Z",
     "iopub.status.idle": "2025-03-26T18:42:29.437491Z",
     "shell.execute_reply": "2025-03-26T18:42:29.436495Z",
     "shell.execute_reply.started": "2025-03-26T18:42:28.925693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Translations:\n",
      "English: Hello, how are you?\n",
      "Arabic: ألو، إيه الأخبار\n",
      "--------------------------------------------------\n",
      "English: The weather today is beautiful.\n",
      "Arabic: الجو النهارده جميل\n",
      "--------------------------------------------------\n",
      "English: Artificial Intelligence is changing the world.\n",
      "Arabic: لا يعني إن العالم كله بيعدي من customicial cream\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_translation(text,model,tokenizer):\n",
    "    # Move the model to the correct device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text and move to the same device as the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate the output using the model\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode the generated text\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate the output using the model\n",
    "    outputs = model.generate(**inputs)\n",
    "    # Decode the generated text\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "# Example predictions\n",
    "examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The weather today is beautiful.\",\n",
    "    \"Artificial Intelligence is changing the world.\",\n",
    "]\n",
    "\n",
    "print(\"Example Translations:\")\n",
    "for text in examples:\n",
    "    translation = generate_translation(text,model,tokenizer)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"Arabic: {translation}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:29.438771Z",
     "iopub.status.busy": "2025-03-26T18:42:29.438431Z",
     "iopub.status.idle": "2025-03-26T18:42:33.411164Z",
     "shell.execute_reply": "2025-03-26T18:42:33.410270Z",
     "shell.execute_reply.started": "2025-03-26T18:42:29.438746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Translations:\n",
      "English: Good morning! How was your weekend?\n",
      "Arabic: صباح الخير، إيه أخبار الويك اند\n",
      "--------------------------------------------------\n",
      "English: I'm feeling great today, thank you!\n",
      "Arabic: أنا حاسس إني تمام النهاردة، شكرا.\n",
      "--------------------------------------------------\n",
      "English: Can you help me with this problem?\n",
      "Arabic: هو أنت ممكن تساعدني في المشكلة دي\n",
      "--------------------------------------------------\n",
      "English: Machine learning algorithms can significantly improve predictive accuracy.\n",
      "Arabic: على فكرة إن التعلب هيعمل coolnessive يحسن.\n",
      "--------------------------------------------------\n",
      "English: The software update includes several bug fixes and performance improvements.\n",
      "Arabic: في clientie clientes، collections و climetings.\n",
      "--------------------------------------------------\n",
      "English: The president addressed the nation last night regarding the economic crisis.\n",
      "Arabic: جو الصباحية، الوردة خاطبوا جوه جوة جوه الأزمة.\n",
      "--------------------------------------------------\n",
      "English: Scientists have discovered a new species of fish in the Amazon River.\n",
      "Arabic: لا إنتوا ابتديتوا تشوفو أنواع جديده من السمك في جوة الأسفلت\n",
      "--------------------------------------------------\n",
      "English: The early bird catches the worm.\n",
      "Arabic: وآخر واحد بياخد منابه\n",
      "--------------------------------------------------\n",
      "English: Actions speak louder than words.\n",
      "Arabic: لو كان بيكلمني بقا هيتكلم كمان أكثر من مجرد كلام.\n",
      "--------------------------------------------------\n",
      "English: Knowledge is power.\n",
      "Arabic: لا، لا، هو عايز قوة.\n",
      "--------------------------------------------------\n",
      "English: The water cycle involves evaporation, condensation, and precipitation.\n",
      "Arabic: دايما في الميه بتبخر، وتتكسف، وتدي تزيين.\n",
      "--------------------------------------------------\n",
      "English: Photosynthesis is the process by which plants convert sunlight into energy.\n",
      "Arabic: التركيبة الكحلية دي هي العملية إللي بتدخل فيها زرع النور وزرع النور وزرعه يعمل زي الشمس.\n",
      "--------------------------------------------------\n",
      "English: Quantum physics explores the behavior of particles at the smallest scales.\n",
      "Arabic: في قوانين الدم بتبان على تصرفات الجزيئات في أصغر وقت.\n",
      "--------------------------------------------------\n",
      "English: Genetic engineering allows scientists to modify the DNA of living organisms.\n",
      "Arabic: مهندس ده هو اللي بيخلي الnone of livances.\n",
      "--------------------------------------------------\n",
      "English: Egypt is known for its ancient pyramids and the Nile River.\n",
      "Arabic: الدملكة بتاعتها بتاعت الأهرامات بتاعتها و المنطقة دي\n",
      "--------------------------------------------------\n",
      "English: The traditional dance at Egyptian weddings is vibrant and joyful.\n",
      "Arabic: الرقصة الغجرية بتاعت الأفندي بتاعت الفراعنة هي رقصة more impressive بتاعت مصرية\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    # Casual Conversation\n",
    "    \"Good morning! How was your weekend?\",\n",
    "    \"I'm feeling great today, thank you!\",\n",
    "    \"Can you help me with this problem?\",\n",
    "    \n",
    "    # Technical Text\n",
    "    \"Machine learning algorithms can significantly improve predictive accuracy.\",\n",
    "    \"The software update includes several bug fixes and performance improvements.\",\n",
    "    \n",
    "    # News and Current Events\n",
    "    \"The president addressed the nation last night regarding the economic crisis.\",\n",
    "    \"Scientists have discovered a new species of fish in the Amazon River.\",\n",
    "    \n",
    "    # Quotes and Sayings\n",
    "    \"The early bird catches the worm.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Knowledge is power.\",\n",
    "    \n",
    "    # Educational Content\n",
    "    \"The water cycle involves evaporation, condensation, and precipitation.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
    "    \n",
    "    # Scientific Statements\n",
    "    \"Quantum physics explores the behavior of particles at the smallest scales.\",\n",
    "    \"Genetic engineering allows scientists to modify the DNA of living organisms.\",\n",
    "    \n",
    "    # Cultural References\n",
    "    \"Egypt is known for its ancient pyramids and the Nile River.\",\n",
    "    \"The traditional dance at Egyptian weddings is vibrant and joyful.\",\n",
    "]\n",
    "\n",
    "print(\"Example Translations:\")\n",
    "for text in examples:\n",
    "    translation = generate_translation(text,model,tokenizer)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"Arabic: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:33.412433Z",
     "iopub.status.busy": "2025-03-26T18:42:33.412107Z",
     "iopub.status.idle": "2025-03-26T18:42:36.801847Z",
     "shell.execute_reply": "2025-03-26T18:42:36.800980Z",
     "shell.execute_reply.started": "2025-03-26T18:42:33.412398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Translations:\n",
      "English: Hey! What's up?\n",
      "Arabic: إيه يا علا\n",
      "--------------------------------------------------\n",
      "English: Good morning! Did you sleep well?\n",
      "Arabic: صباح الخير، نمتي كويس\n",
      "--------------------------------------------------\n",
      "English: I'm so happy to see you!\n",
      "Arabic: أنا عايزة أشوفك بجد.\n",
      "--------------------------------------------------\n",
      "English: Can we go to the mall later?\n",
      "Arabic: ممكن بعدين نروح جو الصباحية\n",
      "--------------------------------------------------\n",
      "English: I forgot my phone at home!\n",
      "Arabic: أنا نسيت تليفوني في البيت.\n",
      "--------------------------------------------------\n",
      "English: Let's grab a coffee together.\n",
      "Arabic: يالا بينا ناخد قهوه مع بعض\n",
      "--------------------------------------------------\n",
      "English: Mom made my favorite food today!\n",
      "Arabic: النهاردة ماما عملا الأكل اللي بحبه\n",
      "--------------------------------------------------\n",
      "English: My little brother keeps annoying me!\n",
      "Arabic: أخويا الصغير كل يوم يزعجني.\n",
      "--------------------------------------------------\n",
      "English: Are you coming to the party tonight?\n",
      "Arabic: إنت هترجع الحفلة النهاردة بالليل\n",
      "--------------------------------------------------\n",
      "English: I want a burger with extra cheese.\n",
      "Arabic: أنا عايز البرجر مع الجبنة زيادة\n",
      "--------------------------------------------------\n",
      "English: Do you like spicy food?\n",
      "Arabic: بتحبي الأكل الإزاز\n",
      "--------------------------------------------------\n",
      "English: This pizza is amazing!\n",
      "Arabic: لا على فكرة، دي فاهمة فاهمة.\n",
      "--------------------------------------------------\n",
      "English: Where is the nearest metro station?\n",
      "Arabic: فين محطة السكة الحديد دي\n",
      "--------------------------------------------------\n",
      "English: How much does a taxi to downtown cost?\n",
      "Arabic: جايز يكل كل يوم من داهية جاية وجايب قد إيه\n",
      "--------------------------------------------------\n",
      "English: I need a ticket to Cairo, please.\n",
      "Arabic: لو سمحتي أنا عايز أروح على مصر\n",
      "--------------------------------------------------\n",
      "English: I just posted a new picture on Instagram!\n",
      "Arabic: حطيت صوره جديده على الإنستجرام.\n",
      "--------------------------------------------------\n",
      "English: Can you send me that video?\n",
      "Arabic: ممكن تبعلي فيديو بقى\n",
      "--------------------------------------------------\n",
      "English: My phone battery is almost dead!\n",
      "Arabic: تركيبة التليفون قربت تموت.\n",
      "--------------------------------------------------\n",
      "English: I'm really tired today.\n",
      "Arabic: أنا تعبان النهاردة.\n",
      "--------------------------------------------------\n",
      "English: That movie made me cry!\n",
      "Arabic: والفيلم ده خلاني أعيط.\n",
      "--------------------------------------------------\n",
      "English: I can't stop laughing at this joke!\n",
      "Arabic: هبطل الضحكة دي!\n",
      "--------------------------------------------------\n",
      "English: How much is this dress?\n",
      "Arabic: الفستان ده قد إيه\n",
      "--------------------------------------------------\n",
      "English: Do you accept credit cards?\n",
      "Arabic: أنت عندك ورقة إئتمان\n",
      "--------------------------------------------------\n",
      "English: I got a great discount on my new shoes!\n",
      "Arabic: بس أنا حسيت بجزم كبير.\n",
      "--------------------------------------------------\n",
      "English: It's so hot today!\n",
      "Arabic: الجو سخن النهاردة.\n",
      "--------------------------------------------------\n",
      "English: Let's go to the beach this weekend.\n",
      "Arabic: تعالي بقى نروح الشط في الweekend الجاي.\n",
      "--------------------------------------------------\n",
      "English: It's raining, so I'll stay home.\n",
      "Arabic: أنا عمري ما هبقى في البيت يا ستي\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "examples = [\n",
    "    # Casual Greetings\n",
    "    \"Hey! What's up?\",\n",
    "    \"Good morning! Did you sleep well?\",\n",
    "    \"I'm so happy to see you!\",\n",
    "\n",
    "    # Daily Life Conversations\n",
    "    \"Can we go to the mall later?\",\n",
    "    \"I forgot my phone at home!\",\n",
    "    \"Let's grab a coffee together.\",\n",
    "\n",
    "    # Family & Friends\n",
    "    \"Mom made my favorite food today!\",\n",
    "    \"My little brother keeps annoying me!\",\n",
    "    \"Are you coming to the party tonight?\",\n",
    "\n",
    "    # Food & Ordering\n",
    "    \"I want a burger with extra cheese.\",\n",
    "    \"Do you like spicy food?\",\n",
    "    \"This pizza is amazing!\",\n",
    "\n",
    "    # Travel & Directions\n",
    "    \"Where is the nearest metro station?\",\n",
    "    \"How much does a taxi to downtown cost?\",\n",
    "    \"I need a ticket to Cairo, please.\",\n",
    "\n",
    "    # Social Media & Tech\n",
    "    \"I just posted a new picture on Instagram!\",\n",
    "    \"Can you send me that video?\",\n",
    "    \"My phone battery is almost dead!\",\n",
    "\n",
    "    # Emotions & Feelings\n",
    "    \"I'm really tired today.\",\n",
    "    \"That movie made me cry!\",\n",
    "    \"I can't stop laughing at this joke!\",\n",
    "\n",
    "    # Shopping & Money\n",
    "    \"How much is this dress?\",\n",
    "    \"Do you accept credit cards?\",\n",
    "    \"I got a great discount on my new shoes!\",\n",
    "\n",
    "    # Weather & Plans\n",
    "    \"It's so hot today!\",\n",
    "    \"Let's go to the beach this weekend.\",\n",
    "    \"It's raining, so I'll stay home.\",\n",
    "]\n",
    "\n",
    "\n",
    "print(\"\\nExample Translations:\")\n",
    "for text in examples:\n",
    "    translation = generate_translation(text,model,tokenizer)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"Arabic: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:36.802848Z",
     "iopub.status.busy": "2025-03-26T18:42:36.802642Z",
     "iopub.status.idle": "2025-03-26T18:42:38.548726Z",
     "shell.execute_reply": "2025-03-26T18:42:38.547930Z",
     "shell.execute_reply.started": "2025-03-26T18:42:36.802829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BLEU score</td><td>▁▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇████████████████</td></tr><tr><td>eval/bleu</td><td>▁▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇████████████████</td></tr><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>eval/runtime</td><td>▂▄█▁▃▂▁▁▃▃▂▃▃▃▂▃▃▃▃▂▃▃▃▃▃▃▃▃▂▄▂▃▃▃▂▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▇▅▁█▅▇██▆▆▇▆▆▆▆▆▆▆▆▇▆▆▆▆▆▅▆▆▆▅▇▅▆▆▇▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▅▁█▅▇██▆▆▇▅▆▆▆▆▆▆▆▇▆▆▆▆▆▅▆▅▆▅▇▅▆▆▇▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▇██▇▇▆▆▆▆▆▅▆▅▅▄▄▃▅▃▃▄▄▂▃▃▂▂▃▁▂▁▂▂▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BLEU score</td><td>21.30826</td></tr><tr><td>eval/bleu</td><td>21.30826</td></tr><tr><td>eval/loss</td><td>3.69418</td></tr><tr><td>eval/runtime</td><td>106.7895</td></tr><tr><td>eval/samples_per_second</td><td>24.394</td></tr><tr><td>eval/steps_per_second</td><td>0.768</td></tr><tr><td>total_flos</td><td>2.362819849224192e+16</td></tr><tr><td>train/epoch</td><td>99.45839</td></tr><tr><td>train/global_step</td><td>18300</td></tr><tr><td>train/grad_norm</td><td>353346.84375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6129</td></tr><tr><td>train_loss</td><td>2.01638</td></tr><tr><td>train_runtime</td><td>19411.0994</td></tr><tr><td>train_samples_per_second</td><td>120.766</td></tr><tr><td>train_steps_per_second</td><td>0.943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-paper-32</strong> at: <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/rq5ignby' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/rq5ignby</a><br> View project at: <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250326_131644-rq5ignby/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()\n",
    "\n",
    "# Sync offline runs with wandb using os.system()\n",
    "os.system(\"wandb sync ./wandb/offline-run-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:38.549764Z",
     "iopub.status.busy": "2025-03-26T18:42:38.549467Z",
     "iopub.status.idle": "2025-03-26T18:42:46.945011Z",
     "shell.execute_reply": "2025-03-26T18:42:46.944150Z",
     "shell.execute_reply.started": "2025-03-26T18:42:38.549741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250326_184238-2blxcvbn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/2blxcvbn' target=\"_blank\">vocal-planet-33</a></strong> to <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/2blxcvbn' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/2blxcvbn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact translation_model:latest, 295.08MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from WandB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# # Initialize WandB run\n",
    "# wandb.init(project=\"egyptian-arabic-translation-finetuning\", entity=\"abdelaziz67-ain-shams-university\")\n",
    "\n",
    "# # Download the artifact\n",
    "# artifact = wandb.use_artifact(\"abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/translation_model:latest\", type=\"model\")\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model_l = AutoModelForSeq2SeqLM.from_pretrained(artifact_dir)\n",
    "# tokenizer_l = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "\n",
    "# print(\"Model and tokenizer loaded from WandB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
