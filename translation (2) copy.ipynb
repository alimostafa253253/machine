{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:53:58.064916Z",
     "iopub.status.busy": "2025-03-27T05:53:58.064523Z",
     "iopub.status.idle": "2025-03-27T05:54:04.493912Z",
     "shell.execute_reply": "2025-03-27T05:54:04.492415Z",
     "shell.execute_reply.started": "2025-03-27T05:53:58.064890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in d:\\venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\venv\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in d:\\venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\venv\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\venv\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\venv\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\venv\\lib\\site-packages (from evaluate) (0.29.1)\n",
      "Requirement already satisfied: packaging in d:\\venv\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\venv\\lib\\site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\venv\\lib\\site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:54:04.495885Z",
     "iopub.status.busy": "2025-03-27T05:54:04.495549Z",
     "iopub.status.idle": "2025-03-27T05:54:31.601760Z",
     "shell.execute_reply": "2025-03-27T05:54:31.600484Z",
     "shell.execute_reply.started": "2025-03-27T05:54:04.495854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:54:31.604450Z",
     "iopub.status.busy": "2025-03-27T05:54:31.604110Z",
     "iopub.status.idle": "2025-03-27T05:55:03.444729Z",
     "shell.execute_reply": "2025-03-27T05:55:03.443584Z",
     "shell.execute_reply.started": "2025-03-27T05:54:31.604381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import re\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:03.447164Z",
     "iopub.status.busy": "2025-03-27T05:55:03.446291Z",
     "iopub.status.idle": "2025-03-27T05:55:09.351577Z",
     "shell.execute_reply": "2025-03-27T05:55:09.350461Z",
     "shell.execute_reply.started": "2025-03-27T05:55:03.447132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdelaziz67\u001b[0m (\u001b[33mabdelaziz67-ain-shams-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your WandB API key directly\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4cf3591f262cd568777e73fcda947286ee03b410\"\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# Initialize WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:09.353748Z",
     "iopub.status.busy": "2025-03-27T05:55:09.352719Z",
     "iopub.status.idle": "2025-03-27T05:55:15.457269Z",
     "shell.execute_reply": "2025-03-27T05:55:15.456257Z",
     "shell.execute_reply.started": "2025-03-27T05:55:09.353703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\projects\\shabab_mobtakren\\abdelaziz\\wandb\\run-20250329_170152-ml3b000a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a' target=\"_blank\">peach-forest-6</a></strong> to <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-english-translation-finetuning/runs/ml3b000a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x194d6f512a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ¯ Start a new WandB run\n",
    "wandb.init(\n",
    "    project=\"egyptian-english-translation-finetuning\",\n",
    "    entity=\"abdelaziz67-ain-shams-university\",  # Your WandB username or organization name\n",
    "    config={\n",
    "        \"model_checkpoint\": \"Helsinki-NLP/opus-mt-en-ar\",\n",
    "        # \"model_checkpoint\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "        \"learning_rate\": 4e-5,  # Lower LR for stable fine-tuning\n",
    "        \"batch_size\": 12,  # Increased batch size\n",
    "        \"num_train_epochs\": 35,  # Extended training duration\n",
    "        \"seed\": 42,\n",
    "        \"label_smoothing\": 0.1  # Prevent overconfidence\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.458569Z",
     "iopub.status.busy": "2025-03-27T05:55:15.458264Z",
     "iopub.status.idle": "2025-03-27T05:55:15.463816Z",
     "shell.execute_reply": "2025-03-27T05:55:15.462526Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.458543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.465443Z",
     "iopub.status.busy": "2025-03-27T05:55:15.465049Z",
     "iopub.status.idle": "2025-03-27T05:55:15.485820Z",
     "shell.execute_reply": "2025-03-27T05:55:15.483992Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.465382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸªµ Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.489221Z",
     "iopub.status.busy": "2025-03-27T05:55:15.488892Z",
     "iopub.status.idle": "2025-03-27T05:55:15.516029Z",
     "shell.execute_reply": "2025-03-27T05:55:15.514911Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.489197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "set_seed(wandb.config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:55:15.517666Z",
     "iopub.status.busy": "2025-03-27T05:55:15.517281Z",
     "iopub.status.idle": "2025-03-27T05:55:17.121322Z",
     "shell.execute_reply": "2025-03-27T05:55:17.120359Z",
     "shell.execute_reply.started": "2025-03-27T05:55:15.517639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading dataset from Hugging Face Hub...\n",
      "INFO:__main__:Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['EGY', 'ENG'],\n",
      "        num_rows: 26047\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading dataset from Hugging Face Hub...\")\n",
    "dataset = load_dataset(\"HeshamHaroon/ArzEn-MultiGenre\")\n",
    "logger.info(\"Dataset loaded: %s\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:56:08.401916Z",
     "iopub.status.busy": "2025-03-27T05:56:08.401381Z",
     "iopub.status.idle": "2025-03-27T05:56:08.409259Z",
     "shell.execute_reply": "2025-03-27T05:56:08.407989Z",
     "shell.execute_reply.started": "2025-03-27T05:56:08.401882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['EGY', 'ENG'],\n",
      "        num_rows: 26047\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø­Ù‚ØŸâ€¬', 'ENG': 'Already?'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'EGY': 'â€«Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø© Ø£Ø®Ø±ØªÙƒÙ….â€¬', 'ENG': 'Sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø§ØŒ ÙˆÙ„Ø§ ÙŠÙ‡Ù…Ùƒ.â€¬', 'ENG': 'No problem.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'EGY': 'â€«Ø¨Ø³ Ø§Ù„system down.â€¬', 'ENG': 'The sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'EGY': 'â€«Ø·ÙŠØ¨. Ø®Ù„Ø§Øµ Ø¥Ø­Ù†Ø§ ÙƒØ¯Ø§ Ù‚ÙÙ„Ù†Ø§ Ø§Ù„joint acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'EGY': 'â€«ÙƒÙ„ Ø¯Ù‡ ÙˆØ§Ù„system downØŸâ€¬', 'ENG': 'All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø§ Ø¥Ù†ØªÙŠ Ø¨ØªØµÙ†Ø¹ÙŠ Ø§Ù„Ù…Ø¹Ø¬Ø²Ø§Øª ÙŠØ§ Ø³Ù‡Ø§ Ø·ÙˆÙ„ Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø§ Ø®Ø§Ù„Øµ. Ø¥Ø­Ù†Ø§ Ø¨Ø³ Ø§ØªØ¹ÙˆØ¯Ù†Ø§.â€¬', 'ENG': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'EGY': 'â€«Ù…Ù† ÙƒØªØ± Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ ÙˆÙ…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù†ÙÙ‚Ø©.â€¬ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'EGY': 'â€«Ø£Ù†Ø§ Ø¨Ø³ Ø¹Ø´Ø§Ù† Ø¹Ø§Ø±Ù Ø¥Ù† Ø¹Ù„Ø§ Ù…Ø´ Ø¨ØªØ´ØªØºÙ„â€¬ â€«...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'EGY': 'â€«Ø£Ù†Ø§ ÙƒÙ…Ø§Ù† Ø£Ù†Ø§ Ø¹Ø±Ø¶Øª Ø¹Ù„ÙŠÙ‡Ø§ Ù…ØµØ±ÙˆÙ Ø´Ø®ØµÙŠ Ø´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ùˆ Ù‡ÙŠ ØªØ­Ø¨ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ø§ Ù…Ù…ÙƒÙ† Ø£Ø¹Ù…Ù„ ÙƒØ¯Ø§â€¬ØŒ Ù…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'EGY': 'â€«ÙˆØ£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ø¥Ù† Ø£Ù†Ø§ Ù…Ø´ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙâ€¬ â€«Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'EGY': 'â€«Ø£Ù†Ø§ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙ Ø§Ù„ÙˆÙ„Ø§Ø¯ ÙˆØ¨Ø³.â€¬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'EGY': 'â€«ÙŠØ¹Ù†ÙŠ Ù‡ØªØµØ±ÙÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³Ùƒ Ù…Ù†ÙŠÙ†ØŸâ€¬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'EGY': 'â€«Ù‡Ø´ØªØºÙ„.â€¬', 'ENG': 'I'll work.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'EGY': 'Ù‡ØªØ´ØªØºÙ„ÙŠ Ø¥ÙŠÙ‡ ÙŠØ§ Ø¹Ù„Ø§ØŸâ€¬', 'ENG': 'Doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'EGY': 'â€«Ø¯Ù‡ fresh graduates Ù…Ø´ Ù„Ø§Ù‚ÙŠÙŠÙ†.â€¬', 'EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'EGY': 'â€«Ù…Ø´ Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„ÙƒØŸâ€¬ â€«Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ù…Ù† Ø§Ù„Ø£ÙˆÙ„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'EGY': 'â€«Ù‡Ùˆ Ù…Ø´ Ø§Ø¨Ù† Ø®Ø§Ù„ØªÙŠØŸ Ø¨Ø³ Ù…ÙÙŠØ´ Ø±Ø§Ø¬Ù„ ÙŠØ³ØªØ§Ù‡Ù„...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                train\n",
       "0                {'EGY': 'â€«Ù„Ø­Ù‚ØŸâ€¬', 'ENG': 'Already?'}\n",
       "1   {'EGY': 'â€«Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø© Ø£Ø®Ø±ØªÙƒÙ….â€¬', 'ENG': 'Sor...\n",
       "2    {'EGY': 'â€«Ù„Ø§ØŒ ÙˆÙ„Ø§ ÙŠÙ‡Ù…Ùƒ.â€¬', 'ENG': 'No problem.'}\n",
       "3   {'EGY': 'â€«Ø¨Ø³ Ø§Ù„system down.â€¬', 'ENG': 'The sys...\n",
       "4   {'EGY': 'â€«Ø·ÙŠØ¨. Ø®Ù„Ø§Øµ Ø¥Ø­Ù†Ø§ ÙƒØ¯Ø§ Ù‚ÙÙ„Ù†Ø§ Ø§Ù„joint acc...\n",
       "5   {'EGY': 'â€«ÙƒÙ„ Ø¯Ù‡ ÙˆØ§Ù„system downØŸâ€¬', 'ENG': 'All...\n",
       "6   {'EGY': 'â€«Ù„Ø§ Ø¥Ù†ØªÙŠ Ø¨ØªØµÙ†Ø¹ÙŠ Ø§Ù„Ù…Ø¹Ø¬Ø²Ø§Øª ÙŠØ§ Ø³Ù‡Ø§ Ø·ÙˆÙ„ Ø¹...\n",
       "7   {'EGY': 'â€«Ù„Ø§ Ø®Ø§Ù„Øµ. Ø¥Ø­Ù†Ø§ Ø¨Ø³ Ø§ØªØ¹ÙˆØ¯Ù†Ø§.â€¬', 'ENG': ...\n",
       "8   {'EGY': 'â€«Ù…Ù† ÙƒØªØ± Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ ÙˆÙ…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù†ÙÙ‚Ø©.â€¬ ...\n",
       "9   {'EGY': 'â€«Ø£Ù†Ø§ Ø¨Ø³ Ø¹Ø´Ø§Ù† Ø¹Ø§Ø±Ù Ø¥Ù† Ø¹Ù„Ø§ Ù…Ø´ Ø¨ØªØ´ØªØºÙ„â€¬ â€«...\n",
       "10  {'EGY': 'â€«Ø£Ù†Ø§ ÙƒÙ…Ø§Ù† Ø£Ù†Ø§ Ø¹Ø±Ø¶Øª Ø¹Ù„ÙŠÙ‡Ø§ Ù…ØµØ±ÙˆÙ Ø´Ø®ØµÙŠ Ø´...\n",
       "11  {'EGY': 'â€«Ù„Ùˆ Ù‡ÙŠ ØªØ­Ø¨ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ø§ Ù…Ù…ÙƒÙ† Ø£Ø¹Ù…Ù„ ÙƒØ¯Ø§â€¬ØŒ Ù…...\n",
       "12  {'EGY': 'â€«ÙˆØ£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ø¥Ù† Ø£Ù†Ø§ Ù…Ø´ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙâ€¬ â€«Ø¹...\n",
       "13  {'EGY': 'â€«Ø£Ù†Ø§ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙ Ø§Ù„ÙˆÙ„Ø§Ø¯ ÙˆØ¨Ø³.â€¬', 'ENG':...\n",
       "14  {'EGY': 'â€«ÙŠØ¹Ù†ÙŠ Ù‡ØªØµØ±ÙÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³Ùƒ Ù…Ù†ÙŠÙ†ØŸâ€¬', 'ENG':...\n",
       "15           {'EGY': 'â€«Ù‡Ø´ØªØºÙ„.â€¬', 'ENG': 'I'll work.'}\n",
       "16  {'EGY': 'Ù‡ØªØ´ØªØºÙ„ÙŠ Ø¥ÙŠÙ‡ ÙŠØ§ Ø¹Ù„Ø§ØŸâ€¬', 'ENG': 'Doing ...\n",
       "17  {'EGY': 'â€«Ø¯Ù‡ fresh graduates Ù…Ø´ Ù„Ø§Ù‚ÙŠÙŠÙ†.â€¬', 'EN...\n",
       "18  {'EGY': 'â€«Ù…Ø´ Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„ÙƒØŸâ€¬ â€«Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ù…Ù† Ø§Ù„Ø£ÙˆÙ„...\n",
       "19  {'EGY': 'â€«Ù‡Ùˆ Ù…Ø´ Ø§Ø¨Ù† Ø®Ø§Ù„ØªÙŠØŸ Ø¨Ø³ Ù…ÙÙŠØ´ Ø±Ø§Ø¬Ù„ ÙŠØ³ØªØ§Ù‡Ù„..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the cleaned dataset\n",
    "print(dataset)\n",
    "import pandas as pd\n",
    "dt=pd.DataFrame(dataset)\n",
    "dt.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:56:40.017931Z",
     "iopub.status.busy": "2025-03-27T05:56:40.017480Z",
     "iopub.status.idle": "2025-03-27T05:56:40.026027Z",
     "shell.execute_reply": "2025-03-27T05:56:40.024055Z",
     "shell.execute_reply.started": "2025-03-27T05:56:40.017903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,!?Ø›ØŒ]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T05:56:46.404637Z",
     "iopub.status.busy": "2025-03-27T05:56:46.404140Z",
     "iopub.status.idle": "2025-03-27T05:56:46.419431Z",
     "shell.execute_reply": "2025-03-27T05:56:46.418098Z",
     "shell.execute_reply.started": "2025-03-27T05:56:46.404603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['EGY', 'ENG'],\n",
      "        num_rows: 22466\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a value is None or only numbers\n",
    "def is_invalid(value):\n",
    "    return value is None or (isinstance(value, str) and value.strip().isdigit())\n",
    "\n",
    "# Function to filter dataset\n",
    "def filter_rows(example):\n",
    "    return not any(is_invalid(v) for v in example.values())\n",
    "\n",
    "# Apply filter to all splits\n",
    "dataset_cleaned = dataset.filter(filter_rows)\n",
    "\n",
    "# Check the cleaned dataset\n",
    "print(dataset_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø­Ù‚ØŸâ€¬', 'ENG': 'Already?'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'EGY': 'â€«Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø© Ø£Ø®Ø±ØªÙƒÙ….â€¬', 'ENG': 'Sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø§ØŒ ÙˆÙ„Ø§ ÙŠÙ‡Ù…Ùƒ.â€¬', 'ENG': 'No problem.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'EGY': 'â€«Ø¨Ø³ Ø§Ù„system down.â€¬', 'ENG': 'The sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'EGY': 'â€«Ø·ÙŠØ¨. Ø®Ù„Ø§Øµ Ø¥Ø­Ù†Ø§ ÙƒØ¯Ø§ Ù‚ÙÙ„Ù†Ø§ Ø§Ù„joint acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'EGY': 'â€«ÙƒÙ„ Ø¯Ù‡ ÙˆØ§Ù„system downØŸâ€¬', 'ENG': 'All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø§ Ø¥Ù†ØªÙŠ Ø¨ØªØµÙ†Ø¹ÙŠ Ø§Ù„Ù…Ø¹Ø¬Ø²Ø§Øª ÙŠØ§ Ø³Ù‡Ø§ Ø·ÙˆÙ„ Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ø§ Ø®Ø§Ù„Øµ. Ø¥Ø­Ù†Ø§ Ø¨Ø³ Ø§ØªØ¹ÙˆØ¯Ù†Ø§.â€¬', 'ENG': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'EGY': 'â€«Ù…Ù† ÙƒØªØ± Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ ÙˆÙ…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù†ÙÙ‚Ø©.â€¬ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'EGY': 'â€«Ø£Ù†Ø§ Ø¨Ø³ Ø¹Ø´Ø§Ù† Ø¹Ø§Ø±Ù Ø¥Ù† Ø¹Ù„Ø§ Ù…Ø´ Ø¨ØªØ´ØªØºÙ„â€¬ â€«...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'EGY': 'â€«Ø£Ù†Ø§ ÙƒÙ…Ø§Ù† Ø£Ù†Ø§ Ø¹Ø±Ø¶Øª Ø¹Ù„ÙŠÙ‡Ø§ Ù…ØµØ±ÙˆÙ Ø´Ø®ØµÙŠ Ø´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'EGY': 'â€«Ù„Ùˆ Ù‡ÙŠ ØªØ­Ø¨ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ø§ Ù…Ù…ÙƒÙ† Ø£Ø¹Ù…Ù„ ÙƒØ¯Ø§â€¬ØŒ Ù…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'EGY': 'â€«ÙˆØ£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ø¥Ù† Ø£Ù†Ø§ Ù…Ø´ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙâ€¬ â€«Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'EGY': 'â€«Ø£Ù†Ø§ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙ Ø§Ù„ÙˆÙ„Ø§Ø¯ ÙˆØ¨Ø³.â€¬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'EGY': 'â€«ÙŠØ¹Ù†ÙŠ Ù‡ØªØµØ±ÙÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³Ùƒ Ù…Ù†ÙŠÙ†ØŸâ€¬', 'ENG':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'EGY': 'â€«Ù‡Ø´ØªØºÙ„.â€¬', 'ENG': 'I'll work.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'EGY': 'Ù‡ØªØ´ØªØºÙ„ÙŠ Ø¥ÙŠÙ‡ ÙŠØ§ Ø¹Ù„Ø§ØŸâ€¬', 'ENG': 'Doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'EGY': 'â€«Ø¯Ù‡ fresh graduates Ù…Ø´ Ù„Ø§Ù‚ÙŠÙŠÙ†.â€¬', 'EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'EGY': 'â€«Ù…Ø´ Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„ÙƒØŸâ€¬ â€«Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ù…Ù† Ø§Ù„Ø£ÙˆÙ„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'EGY': 'â€«Ù‡Ùˆ Ù…Ø´ Ø§Ø¨Ù† Ø®Ø§Ù„ØªÙŠØŸ Ø¨Ø³ Ù…ÙÙŠØ´ Ø±Ø§Ø¬Ù„ ÙŠØ³ØªØ§Ù‡Ù„...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                train\n",
       "0                {'EGY': 'â€«Ù„Ø­Ù‚ØŸâ€¬', 'ENG': 'Already?'}\n",
       "1   {'EGY': 'â€«Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø© Ø£Ø®Ø±ØªÙƒÙ….â€¬', 'ENG': 'Sor...\n",
       "2    {'EGY': 'â€«Ù„Ø§ØŒ ÙˆÙ„Ø§ ÙŠÙ‡Ù…Ùƒ.â€¬', 'ENG': 'No problem.'}\n",
       "3   {'EGY': 'â€«Ø¨Ø³ Ø§Ù„system down.â€¬', 'ENG': 'The sys...\n",
       "4   {'EGY': 'â€«Ø·ÙŠØ¨. Ø®Ù„Ø§Øµ Ø¥Ø­Ù†Ø§ ÙƒØ¯Ø§ Ù‚ÙÙ„Ù†Ø§ Ø§Ù„joint acc...\n",
       "5   {'EGY': 'â€«ÙƒÙ„ Ø¯Ù‡ ÙˆØ§Ù„system downØŸâ€¬', 'ENG': 'All...\n",
       "6   {'EGY': 'â€«Ù„Ø§ Ø¥Ù†ØªÙŠ Ø¨ØªØµÙ†Ø¹ÙŠ Ø§Ù„Ù…Ø¹Ø¬Ø²Ø§Øª ÙŠØ§ Ø³Ù‡Ø§ Ø·ÙˆÙ„ Ø¹...\n",
       "7   {'EGY': 'â€«Ù„Ø§ Ø®Ø§Ù„Øµ. Ø¥Ø­Ù†Ø§ Ø¨Ø³ Ø§ØªØ¹ÙˆØ¯Ù†Ø§.â€¬', 'ENG': ...\n",
       "8   {'EGY': 'â€«Ù…Ù† ÙƒØªØ± Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ ÙˆÙ…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù†ÙÙ‚Ø©.â€¬ ...\n",
       "9   {'EGY': 'â€«Ø£Ù†Ø§ Ø¨Ø³ Ø¹Ø´Ø§Ù† Ø¹Ø§Ø±Ù Ø¥Ù† Ø¹Ù„Ø§ Ù…Ø´ Ø¨ØªØ´ØªØºÙ„â€¬ â€«...\n",
       "10  {'EGY': 'â€«Ø£Ù†Ø§ ÙƒÙ…Ø§Ù† Ø£Ù†Ø§ Ø¹Ø±Ø¶Øª Ø¹Ù„ÙŠÙ‡Ø§ Ù…ØµØ±ÙˆÙ Ø´Ø®ØµÙŠ Ø´...\n",
       "11  {'EGY': 'â€«Ù„Ùˆ Ù‡ÙŠ ØªØ­Ø¨ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ø§ Ù…Ù…ÙƒÙ† Ø£Ø¹Ù…Ù„ ÙƒØ¯Ø§â€¬ØŒ Ù…...\n",
       "12  {'EGY': 'â€«ÙˆØ£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ø¥Ù† Ø£Ù†Ø§ Ù…Ø´ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙâ€¬ â€«Ø¹...\n",
       "13  {'EGY': 'â€«Ø£Ù†Ø§ Ù‡Ø§Ø®Ø¯ Ù…ØµØ±ÙˆÙ Ø§Ù„ÙˆÙ„Ø§Ø¯ ÙˆØ¨Ø³.â€¬', 'ENG':...\n",
       "14  {'EGY': 'â€«ÙŠØ¹Ù†ÙŠ Ù‡ØªØµØ±ÙÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³Ùƒ Ù…Ù†ÙŠÙ†ØŸâ€¬', 'ENG':...\n",
       "15           {'EGY': 'â€«Ù‡Ø´ØªØºÙ„.â€¬', 'ENG': 'I'll work.'}\n",
       "16  {'EGY': 'Ù‡ØªØ´ØªØºÙ„ÙŠ Ø¥ÙŠÙ‡ ÙŠØ§ Ø¹Ù„Ø§ØŸâ€¬', 'ENG': 'Doing ...\n",
       "17  {'EGY': 'â€«Ø¯Ù‡ fresh graduates Ù…Ø´ Ù„Ø§Ù‚ÙŠÙŠÙ†.â€¬', 'EN...\n",
       "18  {'EGY': 'â€«Ù…Ø´ Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„ÙƒØŸâ€¬ â€«Ø£Ù†Ø§ Ù‚ÙˆÙ„ØªÙ„Ùƒ Ù…Ù† Ø§Ù„Ø£ÙˆÙ„...\n",
       "19  {'EGY': 'â€«Ù‡Ùˆ Ù…Ø´ Ø§Ø¨Ù† Ø®Ø§Ù„ØªÙŠØŸ Ø¨Ø³ Ù…ÙÙŠØ´ Ø±Ø§Ø¬Ù„ ÙŠØ³ØªØ§Ù‡Ù„..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(dataset_cleaned)\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T06:00:41.492441Z",
     "iopub.status.busy": "2025-03-27T06:00:41.491940Z",
     "iopub.status.idle": "2025-03-27T06:00:41.521724Z",
     "shell.execute_reply": "2025-03-27T06:00:41.520072Z",
     "shell.execute_reply.started": "2025-03-27T06:00:41.492385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets if not already split.\n",
    "if \"train\" not in dataset_cleaned.keys() or \"validation\" not in dataset_cleaned.keys():\n",
    "    dataset_cleaned = dataset_cleaned[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = dataset_cleaned[\"train\"]\n",
    "    val_dataset = dataset_cleaned[\"test\"]\n",
    "else:\n",
    "    train_dataset = dataset_cleaned[\"train\"]\n",
    "    val_dataset = dataset_cleaned[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T06:00:44.968039Z",
     "iopub.status.busy": "2025-03-27T06:00:44.967599Z",
     "iopub.status.idle": "2025-03-27T06:00:45.026600Z",
     "shell.execute_reply": "2025-03-27T06:00:45.025355Z",
     "shell.execute_reply.started": "2025-03-27T06:00:44.968008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Source: Ø¨ØµÙ„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØºØ±ÙŠØ¨Ø©ØŒ Ø²ÙŠ Ù…Ø§ Ø£ÙƒÙˆÙ† Ø®Ù„ÙŠØªÙ‡ ÙŠØ­Ø³ Ø¨Ø´ÙˆÙŠØ© Ø¥Ø´Ù…Ø¦Ø²Ø§Ø². ÙˆÙ‚Ø§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£Ù‚Ø±Ø¨ Ù„Ù„Ø¹Ù†Ù Ø¥Ù† ÙÙŠ ÙƒÙ„ Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø´Ù‡Ø§Ø¯Ø© Ù…Ø¯ÙŠØ± ÙˆÙ…ÙˆØ¸ÙÙŠÙ† Ø§Ù„Ø¯Ø§Ø± Ù‡ØªØªØ³Ù…Ø¹ ÙˆØ¥Ù† \"Ø¯Ù‡ Ù…Ù…ÙƒÙ† ÙŠÙ‚Ù„Ø¨ Ø¹Ù„ÙŠØ§ Ø§Ù„ØªØ±Ø¨ÙŠØ²Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ­Ø´Ø©\". \n",
      "Target: He gave me a queer look, as if I slightly revolted him; then informed me, in an almost hostile tone, that in any case the head of the Home and some of the staff would be cited as witnesses. And that might do you a very nasty turn, he concluded. \n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Source: Ù…ÙƒÙ†Ø´ Ù…ÙØ±ÙˆØ¶ Ø£Ù‚ÙˆÙ„ Ø§Ù„Ù„ÙŠ Ù‚ÙˆÙ„ØªÙ‡ Ø¯Ù‡ Ø®Ø§Ù„Øµ.\n",
      "Target: I shouldn't have said those things.\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Source: Ø§Ø·Ù„Ø¹ÙŠÙ„ÙŠ ÙÙˆÙ‚ Ø¹Ø´Ø§Ù† Ø£Ø±Ø´Ù‚ Ø§Ù„Ø´ÙˆÙƒÙ‡ ÙÙŠÙƒÙŠ. \n",
      "Target: Come up easy and let me put the harpoon into you.\n",
      "--------------------------------------------------\n",
      "Example 4:\n",
      "Source: Ø±ÙØ¹Øª\n",
      "Target: Refaat!\n",
      "--------------------------------------------------\n",
      "Example 5:\n",
      "Source: Ø§Ù„Ù…Ø±ÙƒØ¨ ÙƒØ§Ù†Øª Ø®ÙÙŠÙÙ‡ Ø¯Ù„ÙˆÙ‚ØªÙŠ ÙˆÙ…Ø§ÙƒØ§Ù†Ø´ Ø¹Ù†Ø¯Ù‡ Ø£ÙŠ Ø£ÙÙƒØ§Ø± ÙˆÙ„Ø§ Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø£ÙŠ Ù†ÙˆØ¹. \n",
      "Target: He sailed lightly now and he had no thoughts nor any feelings of any kind.\n",
      "--------------------------------------------------\n",
      "Example 6:\n",
      "Source: Ø¨Ù‚ÙŠØª Ø´ÙƒØ±ÙŠ Ø³Ø±Ø­Ø§Ù†\n",
      "Target: I've become as famous as Shoukry Sarhan.\n",
      "--------------------------------------------------\n",
      "Example 7:\n",
      "Source: Ù‡Ø´ÙŠÙ„ Ø­Ø§Ø¬Ø§ØªÙŠ Ø¨Ø³ ÙÙŠ Ø«Ø§Ù†ÙŠØ©.\n",
      "Target: I'll just move my stuff. Hold on.\n",
      "--------------------------------------------------\n",
      "Example 8:\n",
      "Source: ÙˆØ¯ÙŠ Ù…Ø´ Ø¨Ø¥ÙŠØ¯ÙŠ\n",
      "Target: It is out of my hands\n",
      "--------------------------------------------------\n",
      "Example 9:\n",
      "Source: Ù†Ø¬Ø±Ø¨ Ø¥ÙŠÙ‡ØŸ \n",
      "Target: Hell, no!\n",
      "--------------------------------------------------\n",
      "Example 10:\n",
      "Source: ÙƒØ§Ù† Ø´Ø§ÙŠÙ„ Ù‚Ø§ÙŠÙ…Ù‡ Ø¨Ø®ÙŠÙ„ Ø§Ù„Ø³Ø¨Ù‚ ÙÙŠ Ø¬ÙŠØ¨Ù‡ Ø·ÙˆÙ„ Ø§Ù„ÙˆÙ‚Øª ÙˆÙƒØ§Ù† Ø¨ÙŠÙ‚ÙˆÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø­ØµÙ†Ù‡ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ„ÙŠÙÙˆÙ† ÙƒØªÙŠØ±.\n",
      "Target: At least he carried lists of horses at all times in his pocket and frequently spoke the names of horses on the telephone. \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show a few examples (e.g., first 5)\n",
    "for idx, example in enumerate(train_dataset.select(range(100,110))):\n",
    "    print(f\"Example {idx+1}:\")\n",
    "    print(\"Source:\", example.get(\"EGY\", \"Field 'tgt' not found\"))\n",
    "    print(\"Target:\", example.get(\"ENG\", \"Field 'src' not found\"))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:52.732652Z",
     "iopub.status.busy": "2025-03-26T13:16:52.732340Z",
     "iopub.status.idle": "2025-03-26T13:16:52.736968Z",
     "shell.execute_reply": "2025-03-26T13:16:52.736107Z",
     "shell.execute_reply.started": "2025-03-26T13:16:52.732621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset loaded successfully!\n",
      "INFO:__main__:Training samples: 20219\n",
      "INFO:__main__:Validation samples: 2247\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Dataset loaded successfully!\")\n",
    "logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:52.738320Z",
     "iopub.status.busy": "2025-03-26T13:16:52.737817Z",
     "iopub.status.idle": "2025-03-26T13:16:57.368173Z",
     "shell.execute_reply": "2025-03-26T13:16:57.366217Z",
     "shell.execute_reply.started": "2025-03-26T13:16:52.738285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Choose a pre-trained translation model\n",
    "model_checkpoint = wandb.config.model_checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.369957Z",
     "iopub.status.busy": "2025-03-26T13:16:57.369610Z",
     "iopub.status.idle": "2025-03-26T13:16:57.375094Z",
     "shell.execute_reply": "2025-03-26T13:16:57.374297Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.369928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.config.dropout = 0.2\n",
    "model.config.attention_dropout = 0.2\n",
    "model.config.activation_dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.376279Z",
     "iopub.status.busy": "2025-03-26T13:16:57.375978Z",
     "iopub.status.idle": "2025-03-26T13:16:57.396590Z",
     "shell.execute_reply": "2025-03-26T13:16:57.396013Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.376249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define source and target languages (adjust codes as needed)\n",
    "source_lang = \"ar_EG\" # for English (if translating from English to Egyptian Arabic)\n",
    "target_lang = \"en_XX\"# Egyptian Arabic variant (if supported)\n",
    "# Note: If your target dialect is not explicitly supported, you can still fine-tune the model\n",
    "# with your dialect-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.397827Z",
     "iopub.status.busy": "2025-03-26T13:16:57.397561Z",
     "iopub.status.idle": "2025-03-26T13:16:57.417030Z",
     "shell.execute_reply": "2025-03-26T13:16:57.416131Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.397806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Ensure inputs are strings\n",
    "    inputs = [clean_text(str(text)) for text in examples[\"EGY\"]]\n",
    "    targets = [clean_text(str(text)) for text in examples[\"ENG\"]]\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # Tokenize the target texts using the new text_target argument\n",
    "    labels = tokenizer(text_target=targets, max_length=max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Before tokenizing, set the source and target language codes on the tokenizer.\n",
    "# For mBART50, for example:\n",
    "tokenizer.src_lang = \"ar_EG\"  # assuming source is English\n",
    "tokenizer.tgt_lang = \"en_XX\"  # assuming target is Arabic; change if you have a dialect-specific code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:16:57.418228Z",
     "iopub.status.busy": "2025-03-26T13:16:57.417880Z",
     "iopub.status.idle": "2025-03-26T13:17:03.532391Z",
     "shell.execute_reply": "2025-03-26T13:17:03.531399Z",
     "shell.execute_reply.started": "2025-03-26T13:16:57.418192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:03.533423Z",
     "iopub.status.busy": "2025-03-26T13:17:03.533206Z",
     "iopub.status.idle": "2025-03-26T13:17:03.537336Z",
     "shell.execute_reply": "2025-03-26T13:17:03.536578Z",
     "shell.execute_reply.started": "2025-03-26T13:17:03.533404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:03.538411Z",
     "iopub.status.busy": "2025-03-26T13:17:03.538182Z",
     "iopub.status.idle": "2025-03-26T13:17:03.713884Z",
     "shell.execute_reply": "2025-03-26T13:17:03.713193Z",
     "shell.execute_reply.started": "2025-03-26T13:17:03.538392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,  # Keep only the last 2 checkpoints,\n",
    "    save_only_model=True,   # Save only the model\n",
    "    per_device_train_batch_size=wandb.config.batch_size,\n",
    "    per_device_eval_batch_size=wandb.config.batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=wandb.config.num_train_epochs,\n",
    "    learning_rate=wandb.config.learning_rate,\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate decay,\n",
    "    weight_decay=0.03,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,  # Limit generation length\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    label_smoothing_factor=wandb.config.label_smoothing,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"optimized-egyptian-arabic-translation\",\n",
    "    fp16=True  # Enable mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:03.714973Z",
     "iopub.status.busy": "2025-03-26T13:17:03.714774Z",
     "iopub.status.idle": "2025-03-26T13:17:07.679726Z",
     "shell.execute_reply": "2025-03-26T13:17:07.678814Z",
     "shell.execute_reply.started": "2025-03-26T13:17:03.714955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in d:\\venv\\lib\\site-packages (from sacrebleu) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\venv\\lib\\site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in d:\\venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in d:\\venv\\lib\\site-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\venv\\lib\\site-packages (from portalocker->sacrebleu) (308)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:07.681332Z",
     "iopub.status.busy": "2025-03-26T13:17:07.680928Z",
     "iopub.status.idle": "2025-03-26T13:17:08.175903Z",
     "shell.execute_reply": "2025-03-26T13:17:08.175139Z",
     "shell.execute_reply.started": "2025-03-26T13:17:07.681280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the sacreBLEU metric using the evaluate library.\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.177763Z",
     "iopub.status.busy": "2025-03-26T13:17:08.176677Z",
     "iopub.status.idle": "2025-03-26T13:17:08.183753Z",
     "shell.execute_reply": "2025-03-26T13:17:08.182768Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.177736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Safely convert predictions to integers\n",
    "    preds = preds.astype(object)\n",
    "    labels = labels.astype(object)\n",
    "\n",
    "    # Decode the generated texts and labels directly using the tokenizer\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace all -100 in labels with the padding token ID\n",
    "    labels = [[(int(token) if token >= 0 else tokenizer.pad_token_id) for token in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Prepare references in the required format\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    # Compute BLEU score using sacrebleu\n",
    "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Log BLEU score to wandb\n",
    "    wandb.log({\"BLEU score\": result[\"score\"]})\n",
    "\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.185022Z",
     "iopub.status.busy": "2025-03-26T13:17:08.184705Z",
     "iopub.status.idle": "2025-03-26T13:17:08.207754Z",
     "shell.execute_reply": "2025-03-26T13:17:08.206971Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.184991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.208630Z",
     "iopub.status.busy": "2025-03-26T13:17:08.208414Z",
     "iopub.status.idle": "2025-03-26T13:17:08.226887Z",
     "shell.execute_reply": "2025-03-26T13:17:08.226063Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.208611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.227803Z",
     "iopub.status.busy": "2025-03-26T13:17:08.227618Z",
     "iopub.status.idle": "2025-03-26T13:17:08.245039Z",
     "shell.execute_reply": "2025-03-26T13:17:08.244270Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.227786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def clear_memory():\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# # Call this after each evaluation step or at the end of training\n",
    "# clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.248866Z",
     "iopub.status.busy": "2025-03-26T13:17:08.248670Z",
     "iopub.status.idle": "2025-03-26T13:17:08.612267Z",
     "shell.execute_reply": "2025-03-26T13:17:08.611547Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.248849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,  # Already handles tokenization and padding\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['EGY', 'ENG', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2247\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T13:17:08.613636Z",
     "iopub.status.busy": "2025-03-26T13:17:08.613379Z",
     "iopub.status.idle": "2025-03-26T18:40:40.145026Z",
     "shell.execute_reply": "2025-03-26T18:40:40.144122Z",
     "shell.execute_reply.started": "2025-03-26T13:17:08.613614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='14735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  237/14735 32:55 < 33:50:53, 0.12 it/s, Epoch 0.56/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start fine-tuning\n",
    "logger.info(\"Starting fine-tuning...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:40:40.146308Z",
     "iopub.status.busy": "2025-03-26T18:40:40.145973Z",
     "iopub.status.idle": "2025-03-26T18:42:26.943310Z",
     "shell.execute_reply": "2025-03-26T18:42:26.942663Z",
     "shell.execute_reply.started": "2025-03-26T18:40:40.146256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 01:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "logger.info(\"Evaluating the model on the validation set...\")\n",
    "evaluation_results = trainer.evaluate()\n",
    "logger.info(\"Evaluation results: %s\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:26.944267Z",
     "iopub.status.busy": "2025-03-26T18:42:26.944060Z",
     "iopub.status.idle": "2025-03-26T18:42:27.716695Z",
     "shell.execute_reply": "2025-03-26T18:42:27.716041Z",
     "shell.execute_reply.started": "2025-03-26T18:42:26.944248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "logger.info(\"Model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:27.717640Z",
     "iopub.status.busy": "2025-03-26T18:42:27.717410Z",
     "iopub.status.idle": "2025-03-26T18:42:28.924686Z",
     "shell.execute_reply": "2025-03-26T18:42:28.923927Z",
     "shell.execute_reply.started": "2025-03-26T18:42:27.717619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./final_model)... Done. 1.1s\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Log the model as a WandB artifact\n",
    "artifact = wandb.Artifact(\"translation_model\", type=\"model\")\n",
    "artifact.add_dir(\"./final_model\")\n",
    "wandb.log_artifact(artifact)\n",
    "logger.info(\"Model saved and logged to WandB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:28.925715Z",
     "iopub.status.busy": "2025-03-26T18:42:28.925441Z",
     "iopub.status.idle": "2025-03-26T18:42:29.437491Z",
     "shell.execute_reply": "2025-03-26T18:42:29.436495Z",
     "shell.execute_reply.started": "2025-03-26T18:42:28.925693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Translations:\n",
      "English: Hello, how are you?\n",
      "Arabic: Ø£Ù„ÙˆØŒ Ø¥ÙŠÙ‡ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±\n",
      "--------------------------------------------------\n",
      "English: The weather today is beautiful.\n",
      "Arabic: Ø§Ù„Ø¬Ùˆ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ù‡ Ø¬Ù…ÙŠÙ„\n",
      "--------------------------------------------------\n",
      "English: Artificial Intelligence is changing the world.\n",
      "Arabic: Ù„Ø§ ÙŠØ¹Ù†ÙŠ Ø¥Ù† Ø§Ù„Ø¹Ø§Ù„Ù… ÙƒÙ„Ù‡ Ø¨ÙŠØ¹Ø¯ÙŠ Ù…Ù† customicial cream\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_translation(text,model,tokenizer):\n",
    "    # Move the model to the correct device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text and move to the same device as the model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate the output using the model\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode the generated text\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate the output using the model\n",
    "    outputs = model.generate(**inputs)\n",
    "    # Decode the generated text\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "# Example predictions\n",
    "examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The weather today is beautiful.\",\n",
    "    \"Artificial Intelligence is changing the world.\",\n",
    "]\n",
    "\n",
    "print(\"Example Translations:\")\n",
    "for text in examples:\n",
    "    translation = generate_translation(text,model,tokenizer)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"Arabic: {translation}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:29.438771Z",
     "iopub.status.busy": "2025-03-26T18:42:29.438431Z",
     "iopub.status.idle": "2025-03-26T18:42:33.411164Z",
     "shell.execute_reply": "2025-03-26T18:42:33.410270Z",
     "shell.execute_reply.started": "2025-03-26T18:42:29.438746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Translations:\n",
      "English: Good morning! How was your weekend?\n",
      "Arabic: ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±ØŒ Ø¥ÙŠÙ‡ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆÙŠÙƒ Ø§Ù†Ø¯\n",
      "--------------------------------------------------\n",
      "English: I'm feeling great today, thank you!\n",
      "Arabic: Ø£Ù†Ø§ Ø­Ø§Ø³Ø³ Ø¥Ù†ÙŠ ØªÙ…Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŒ Ø´ÙƒØ±Ø§.\n",
      "--------------------------------------------------\n",
      "English: Can you help me with this problem?\n",
      "Arabic: Ù‡Ùˆ Ø£Ù†Øª Ù…Ù…ÙƒÙ† ØªØ³Ø§Ø¹Ø¯Ù†ÙŠ ÙÙŠ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¯ÙŠ\n",
      "--------------------------------------------------\n",
      "English: Machine learning algorithms can significantly improve predictive accuracy.\n",
      "Arabic: Ø¹Ù„Ù‰ ÙÙƒØ±Ø© Ø¥Ù† Ø§Ù„ØªØ¹Ù„Ø¨ Ù‡ÙŠØ¹Ù…Ù„ coolnessive ÙŠØ­Ø³Ù†.\n",
      "--------------------------------------------------\n",
      "English: The software update includes several bug fixes and performance improvements.\n",
      "Arabic: ÙÙŠ clientie clientesØŒ collections Ùˆ climetings.\n",
      "--------------------------------------------------\n",
      "English: The president addressed the nation last night regarding the economic crisis.\n",
      "Arabic: Ø¬Ùˆ Ø§Ù„ØµØ¨Ø§Ø­ÙŠØ©ØŒ Ø§Ù„ÙˆØ±Ø¯Ø© Ø®Ø§Ø·Ø¨ÙˆØ§ Ø¬ÙˆÙ‡ Ø¬ÙˆØ© Ø¬ÙˆÙ‡ Ø§Ù„Ø£Ø²Ù…Ø©.\n",
      "--------------------------------------------------\n",
      "English: Scientists have discovered a new species of fish in the Amazon River.\n",
      "Arabic: Ù„Ø§ Ø¥Ù†ØªÙˆØ§ Ø§Ø¨ØªØ¯ÙŠØªÙˆØ§ ØªØ´ÙˆÙÙˆ Ø£Ù†ÙˆØ§Ø¹ Ø¬Ø¯ÙŠØ¯Ù‡ Ù…Ù† Ø§Ù„Ø³Ù…Ùƒ ÙÙŠ Ø¬ÙˆØ© Ø§Ù„Ø£Ø³ÙÙ„Øª\n",
      "--------------------------------------------------\n",
      "English: The early bird catches the worm.\n",
      "Arabic: ÙˆØ¢Ø®Ø± ÙˆØ§Ø­Ø¯ Ø¨ÙŠØ§Ø®Ø¯ Ù…Ù†Ø§Ø¨Ù‡\n",
      "--------------------------------------------------\n",
      "English: Actions speak louder than words.\n",
      "Arabic: Ù„Ùˆ ÙƒØ§Ù† Ø¨ÙŠÙƒÙ„Ù…Ù†ÙŠ Ø¨Ù‚Ø§ Ù‡ÙŠØªÙƒÙ„Ù… ÙƒÙ…Ø§Ù† Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø¬Ø±Ø¯ ÙƒÙ„Ø§Ù….\n",
      "--------------------------------------------------\n",
      "English: Knowledge is power.\n",
      "Arabic: Ù„Ø§ØŒ Ù„Ø§ØŒ Ù‡Ùˆ Ø¹Ø§ÙŠØ² Ù‚ÙˆØ©.\n",
      "--------------------------------------------------\n",
      "English: The water cycle involves evaporation, condensation, and precipitation.\n",
      "Arabic: Ø¯Ø§ÙŠÙ…Ø§ ÙÙŠ Ø§Ù„Ù…ÙŠÙ‡ Ø¨ØªØ¨Ø®Ø±ØŒ ÙˆØªØªÙƒØ³ÙØŒ ÙˆØªØ¯ÙŠ ØªØ²ÙŠÙŠÙ†.\n",
      "--------------------------------------------------\n",
      "English: Photosynthesis is the process by which plants convert sunlight into energy.\n",
      "Arabic: Ø§Ù„ØªØ±ÙƒÙŠØ¨Ø© Ø§Ù„ÙƒØ­Ù„ÙŠØ© Ø¯ÙŠ Ù‡ÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¥Ù„Ù„ÙŠ Ø¨ØªØ¯Ø®Ù„ ÙÙŠÙ‡Ø§ Ø²Ø±Ø¹ Ø§Ù„Ù†ÙˆØ± ÙˆØ²Ø±Ø¹ Ø§Ù„Ù†ÙˆØ± ÙˆØ²Ø±Ø¹Ù‡ ÙŠØ¹Ù…Ù„ Ø²ÙŠ Ø§Ù„Ø´Ù…Ø³.\n",
      "--------------------------------------------------\n",
      "English: Quantum physics explores the behavior of particles at the smallest scales.\n",
      "Arabic: ÙÙŠ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø¯Ù… Ø¨ØªØ¨Ø§Ù† Ø¹Ù„Ù‰ ØªØµØ±ÙØ§Øª Ø§Ù„Ø¬Ø²ÙŠØ¦Ø§Øª ÙÙŠ Ø£ØµØºØ± ÙˆÙ‚Øª.\n",
      "--------------------------------------------------\n",
      "English: Genetic engineering allows scientists to modify the DNA of living organisms.\n",
      "Arabic: Ù…Ù‡Ù†Ø¯Ø³ Ø¯Ù‡ Ù‡Ùˆ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ®Ù„ÙŠ Ø§Ù„none of livances.\n",
      "--------------------------------------------------\n",
      "English: Egypt is known for its ancient pyramids and the Nile River.\n",
      "Arabic: Ø§Ù„Ø¯Ù…Ù„ÙƒØ© Ø¨ØªØ§Ø¹ØªÙ‡Ø§ Ø¨ØªØ§Ø¹Øª Ø§Ù„Ø£Ù‡Ø±Ø§Ù…Ø§Øª Ø¨ØªØ§Ø¹ØªÙ‡Ø§ Ùˆ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¯ÙŠ\n",
      "--------------------------------------------------\n",
      "English: The traditional dance at Egyptian weddings is vibrant and joyful.\n",
      "Arabic: Ø§Ù„Ø±Ù‚ØµØ© Ø§Ù„ØºØ¬Ø±ÙŠØ© Ø¨ØªØ§Ø¹Øª Ø§Ù„Ø£ÙÙ†Ø¯ÙŠ Ø¨ØªØ§Ø¹Øª Ø§Ù„ÙØ±Ø§Ø¹Ù†Ø© Ù‡ÙŠ Ø±Ù‚ØµØ© more impressive Ø¨ØªØ§Ø¹Øª Ù…ØµØ±ÙŠØ©\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    # Casual Conversation\n",
    "    \"Good morning! How was your weekend?\",\n",
    "    \"I'm feeling great today, thank you!\",\n",
    "    \"Can you help me with this problem?\",\n",
    "    \n",
    "    # Technical Text\n",
    "    \"Machine learning algorithms can significantly improve predictive accuracy.\",\n",
    "    \"The software update includes several bug fixes and performance improvements.\",\n",
    "    \n",
    "    # News and Current Events\n",
    "    \"The president addressed the nation last night regarding the economic crisis.\",\n",
    "    \"Scientists have discovered a new species of fish in the Amazon River.\",\n",
    "    \n",
    "    # Quotes and Sayings\n",
    "    \"The early bird catches the worm.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Knowledge is power.\",\n",
    "    \n",
    "    # Educational Content\n",
    "    \"The water cycle involves evaporation, condensation, and precipitation.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
    "    \n",
    "    # Scientific Statements\n",
    "    \"Quantum physics explores the behavior of particles at the smallest scales.\",\n",
    "    \"Genetic engineering allows scientists to modify the DNA of living organisms.\",\n",
    "    \n",
    "    # Cultural References\n",
    "    \"Egypt is known for its ancient pyramids and the Nile River.\",\n",
    "    \"The traditional dance at Egyptian weddings is vibrant and joyful.\",\n",
    "]\n",
    "\n",
    "print(\"Example Translations:\")\n",
    "for text in examples:\n",
    "    translation = generate_translation(text,model,tokenizer)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"Arabic: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:33.412433Z",
     "iopub.status.busy": "2025-03-26T18:42:33.412107Z",
     "iopub.status.idle": "2025-03-26T18:42:36.801847Z",
     "shell.execute_reply": "2025-03-26T18:42:36.800980Z",
     "shell.execute_reply.started": "2025-03-26T18:42:33.412398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Translations:\n",
      "English: Hey! What's up?\n",
      "Arabic: Ø¥ÙŠÙ‡ ÙŠØ§ Ø¹Ù„Ø§\n",
      "--------------------------------------------------\n",
      "English: Good morning! Did you sleep well?\n",
      "Arabic: ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±ØŒ Ù†Ù…ØªÙŠ ÙƒÙˆÙŠØ³\n",
      "--------------------------------------------------\n",
      "English: I'm so happy to see you!\n",
      "Arabic: Ø£Ù†Ø§ Ø¹Ø§ÙŠØ²Ø© Ø£Ø´ÙˆÙÙƒ Ø¨Ø¬Ø¯.\n",
      "--------------------------------------------------\n",
      "English: Can we go to the mall later?\n",
      "Arabic: Ù…Ù…ÙƒÙ† Ø¨Ø¹Ø¯ÙŠÙ† Ù†Ø±ÙˆØ­ Ø¬Ùˆ Ø§Ù„ØµØ¨Ø§Ø­ÙŠØ©\n",
      "--------------------------------------------------\n",
      "English: I forgot my phone at home!\n",
      "Arabic: Ø£Ù†Ø§ Ù†Ø³ÙŠØª ØªÙ„ÙŠÙÙˆÙ†ÙŠ ÙÙŠ Ø§Ù„Ø¨ÙŠØª.\n",
      "--------------------------------------------------\n",
      "English: Let's grab a coffee together.\n",
      "Arabic: ÙŠØ§Ù„Ø§ Ø¨ÙŠÙ†Ø§ Ù†Ø§Ø®Ø¯ Ù‚Ù‡ÙˆÙ‡ Ù…Ø¹ Ø¨Ø¹Ø¶\n",
      "--------------------------------------------------\n",
      "English: Mom made my favorite food today!\n",
      "Arabic: Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø© Ù…Ø§Ù…Ø§ Ø¹Ù…Ù„Ø§ Ø§Ù„Ø£ÙƒÙ„ Ø§Ù„Ù„ÙŠ Ø¨Ø­Ø¨Ù‡\n",
      "--------------------------------------------------\n",
      "English: My little brother keeps annoying me!\n",
      "Arabic: Ø£Ø®ÙˆÙŠØ§ Ø§Ù„ØµØºÙŠØ± ÙƒÙ„ ÙŠÙˆÙ… ÙŠØ²Ø¹Ø¬Ù†ÙŠ.\n",
      "--------------------------------------------------\n",
      "English: Are you coming to the party tonight?\n",
      "Arabic: Ø¥Ù†Øª Ù‡ØªØ±Ø¬Ø¹ Ø§Ù„Ø­ÙÙ„Ø© Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø© Ø¨Ø§Ù„Ù„ÙŠÙ„\n",
      "--------------------------------------------------\n",
      "English: I want a burger with extra cheese.\n",
      "Arabic: Ø£Ù†Ø§ Ø¹Ø§ÙŠØ² Ø§Ù„Ø¨Ø±Ø¬Ø± Ù…Ø¹ Ø§Ù„Ø¬Ø¨Ù†Ø© Ø²ÙŠØ§Ø¯Ø©\n",
      "--------------------------------------------------\n",
      "English: Do you like spicy food?\n",
      "Arabic: Ø¨ØªØ­Ø¨ÙŠ Ø§Ù„Ø£ÙƒÙ„ Ø§Ù„Ø¥Ø²Ø§Ø²\n",
      "--------------------------------------------------\n",
      "English: This pizza is amazing!\n",
      "Arabic: Ù„Ø§ Ø¹Ù„Ù‰ ÙÙƒØ±Ø©ØŒ Ø¯ÙŠ ÙØ§Ù‡Ù…Ø© ÙØ§Ù‡Ù…Ø©.\n",
      "--------------------------------------------------\n",
      "English: Where is the nearest metro station?\n",
      "Arabic: ÙÙŠÙ† Ù…Ø­Ø·Ø© Ø§Ù„Ø³ÙƒØ© Ø§Ù„Ø­Ø¯ÙŠØ¯ Ø¯ÙŠ\n",
      "--------------------------------------------------\n",
      "English: How much does a taxi to downtown cost?\n",
      "Arabic: Ø¬Ø§ÙŠØ² ÙŠÙƒÙ„ ÙƒÙ„ ÙŠÙˆÙ… Ù…Ù† Ø¯Ø§Ù‡ÙŠØ© Ø¬Ø§ÙŠØ© ÙˆØ¬Ø§ÙŠØ¨ Ù‚Ø¯ Ø¥ÙŠÙ‡\n",
      "--------------------------------------------------\n",
      "English: I need a ticket to Cairo, please.\n",
      "Arabic: Ù„Ùˆ Ø³Ù…Ø­ØªÙŠ Ø£Ù†Ø§ Ø¹Ø§ÙŠØ² Ø£Ø±ÙˆØ­ Ø¹Ù„Ù‰ Ù…ØµØ±\n",
      "--------------------------------------------------\n",
      "English: I just posted a new picture on Instagram!\n",
      "Arabic: Ø­Ø·ÙŠØª ØµÙˆØ±Ù‡ Ø¬Ø¯ÙŠØ¯Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø³ØªØ¬Ø±Ø§Ù….\n",
      "--------------------------------------------------\n",
      "English: Can you send me that video?\n",
      "Arabic: Ù…Ù…ÙƒÙ† ØªØ¨Ø¹Ù„ÙŠ ÙÙŠØ¯ÙŠÙˆ Ø¨Ù‚Ù‰\n",
      "--------------------------------------------------\n",
      "English: My phone battery is almost dead!\n",
      "Arabic: ØªØ±ÙƒÙŠØ¨Ø© Ø§Ù„ØªÙ„ÙŠÙÙˆÙ† Ù‚Ø±Ø¨Øª ØªÙ…ÙˆØª.\n",
      "--------------------------------------------------\n",
      "English: I'm really tired today.\n",
      "Arabic: Ø£Ù†Ø§ ØªØ¹Ø¨Ø§Ù† Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©.\n",
      "--------------------------------------------------\n",
      "English: That movie made me cry!\n",
      "Arabic: ÙˆØ§Ù„ÙÙŠÙ„Ù… Ø¯Ù‡ Ø®Ù„Ø§Ù†ÙŠ Ø£Ø¹ÙŠØ·.\n",
      "--------------------------------------------------\n",
      "English: I can't stop laughing at this joke!\n",
      "Arabic: Ù‡Ø¨Ø·Ù„ Ø§Ù„Ø¶Ø­ÙƒØ© Ø¯ÙŠ!\n",
      "--------------------------------------------------\n",
      "English: How much is this dress?\n",
      "Arabic: Ø§Ù„ÙØ³ØªØ§Ù† Ø¯Ù‡ Ù‚Ø¯ Ø¥ÙŠÙ‡\n",
      "--------------------------------------------------\n",
      "English: Do you accept credit cards?\n",
      "Arabic: Ø£Ù†Øª Ø¹Ù†Ø¯Ùƒ ÙˆØ±Ù‚Ø© Ø¥Ø¦ØªÙ…Ø§Ù†\n",
      "--------------------------------------------------\n",
      "English: I got a great discount on my new shoes!\n",
      "Arabic: Ø¨Ø³ Ø£Ù†Ø§ Ø­Ø³ÙŠØª Ø¨Ø¬Ø²Ù… ÙƒØ¨ÙŠØ±.\n",
      "--------------------------------------------------\n",
      "English: It's so hot today!\n",
      "Arabic: Ø§Ù„Ø¬Ùˆ Ø³Ø®Ù† Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©.\n",
      "--------------------------------------------------\n",
      "English: Let's go to the beach this weekend.\n",
      "Arabic: ØªØ¹Ø§Ù„ÙŠ Ø¨Ù‚Ù‰ Ù†Ø±ÙˆØ­ Ø§Ù„Ø´Ø· ÙÙŠ Ø§Ù„weekend Ø§Ù„Ø¬Ø§ÙŠ.\n",
      "--------------------------------------------------\n",
      "English: It's raining, so I'll stay home.\n",
      "Arabic: Ø£Ù†Ø§ Ø¹Ù…Ø±ÙŠ Ù…Ø§ Ù‡Ø¨Ù‚Ù‰ ÙÙŠ Ø§Ù„Ø¨ÙŠØª ÙŠØ§ Ø³ØªÙŠ\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "examples = [\n",
    "    # Casual Greetings\n",
    "    \"Hey! What's up?\",\n",
    "    \"Good morning! Did you sleep well?\",\n",
    "    \"I'm so happy to see you!\",\n",
    "\n",
    "    # Daily Life Conversations\n",
    "    \"Can we go to the mall later?\",\n",
    "    \"I forgot my phone at home!\",\n",
    "    \"Let's grab a coffee together.\",\n",
    "\n",
    "    # Family & Friends\n",
    "    \"Mom made my favorite food today!\",\n",
    "    \"My little brother keeps annoying me!\",\n",
    "    \"Are you coming to the party tonight?\",\n",
    "\n",
    "    # Food & Ordering\n",
    "    \"I want a burger with extra cheese.\",\n",
    "    \"Do you like spicy food?\",\n",
    "    \"This pizza is amazing!\",\n",
    "\n",
    "    # Travel & Directions\n",
    "    \"Where is the nearest metro station?\",\n",
    "    \"How much does a taxi to downtown cost?\",\n",
    "    \"I need a ticket to Cairo, please.\",\n",
    "\n",
    "    # Social Media & Tech\n",
    "    \"I just posted a new picture on Instagram!\",\n",
    "    \"Can you send me that video?\",\n",
    "    \"My phone battery is almost dead!\",\n",
    "\n",
    "    # Emotions & Feelings\n",
    "    \"I'm really tired today.\",\n",
    "    \"That movie made me cry!\",\n",
    "    \"I can't stop laughing at this joke!\",\n",
    "\n",
    "    # Shopping & Money\n",
    "    \"How much is this dress?\",\n",
    "    \"Do you accept credit cards?\",\n",
    "    \"I got a great discount on my new shoes!\",\n",
    "\n",
    "    # Weather & Plans\n",
    "    \"It's so hot today!\",\n",
    "    \"Let's go to the beach this weekend.\",\n",
    "    \"It's raining, so I'll stay home.\",\n",
    "]\n",
    "\n",
    "\n",
    "print(\"\\nExample Translations:\")\n",
    "for text in examples:\n",
    "    translation = generate_translation(text,model,tokenizer)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"Arabic: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:36.802848Z",
     "iopub.status.busy": "2025-03-26T18:42:36.802642Z",
     "iopub.status.idle": "2025-03-26T18:42:38.548726Z",
     "shell.execute_reply": "2025-03-26T18:42:38.547930Z",
     "shell.execute_reply.started": "2025-03-26T18:42:36.802829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BLEU score</td><td>â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/bleu</td><td>â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ</td></tr><tr><td>eval/runtime</td><td>â–‚â–„â–ˆâ–â–ƒâ–‚â–â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒ</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–…â–â–ˆâ–…â–‡â–ˆâ–ˆâ–†â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–…â–†â–†â–†â–…â–‡â–…â–†â–†â–‡â–†â–†</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–…â–â–ˆâ–…â–‡â–ˆâ–ˆâ–†â–†â–‡â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–…â–†â–…â–†â–…â–‡â–…â–†â–†â–‡â–†â–†</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–…â–†â–…â–…â–„â–„â–ƒâ–…â–ƒâ–ƒâ–„â–„â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–â–‚â–‚â–â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BLEU score</td><td>21.30826</td></tr><tr><td>eval/bleu</td><td>21.30826</td></tr><tr><td>eval/loss</td><td>3.69418</td></tr><tr><td>eval/runtime</td><td>106.7895</td></tr><tr><td>eval/samples_per_second</td><td>24.394</td></tr><tr><td>eval/steps_per_second</td><td>0.768</td></tr><tr><td>total_flos</td><td>2.362819849224192e+16</td></tr><tr><td>train/epoch</td><td>99.45839</td></tr><tr><td>train/global_step</td><td>18300</td></tr><tr><td>train/grad_norm</td><td>353346.84375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6129</td></tr><tr><td>train_loss</td><td>2.01638</td></tr><tr><td>train_runtime</td><td>19411.0994</td></tr><tr><td>train_samples_per_second</td><td>120.766</td></tr><tr><td>train_steps_per_second</td><td>0.943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-paper-32</strong> at: <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/rq5ignby' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/rq5ignby</a><br> View project at: <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250326_131644-rq5ignby/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()\n",
    "\n",
    "# Sync offline runs with wandb using os.system()\n",
    "os.system(\"wandb sync ./wandb/offline-run-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:42:38.549764Z",
     "iopub.status.busy": "2025-03-26T18:42:38.549467Z",
     "iopub.status.idle": "2025-03-26T18:42:46.945011Z",
     "shell.execute_reply": "2025-03-26T18:42:46.944150Z",
     "shell.execute_reply.started": "2025-03-26T18:42:38.549741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250326_184238-2blxcvbn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/2blxcvbn' target=\"_blank\">vocal-planet-33</a></strong> to <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/2blxcvbn' target=\"_blank\">https://wandb.ai/abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/runs/2blxcvbn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact translation_model:latest, 295.08MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from WandB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# # Initialize WandB run\n",
    "# wandb.init(project=\"egyptian-arabic-translation-finetuning\", entity=\"abdelaziz67-ain-shams-university\")\n",
    "\n",
    "# # Download the artifact\n",
    "# artifact = wandb.use_artifact(\"abdelaziz67-ain-shams-university/egyptian-arabic-translation-finetuning/translation_model:latest\", type=\"model\")\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model_l = AutoModelForSeq2SeqLM.from_pretrained(artifact_dir)\n",
    "# tokenizer_l = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "\n",
    "# print(\"Model and tokenizer loaded from WandB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
